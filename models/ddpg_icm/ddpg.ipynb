{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ddpg.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "klsFNbTF8j62"
      },
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "878QD18d7JDC",
        "outputId": "e38ea4b2-829a-48c6-c39f-b07e3841107f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4185
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install gym\n",
        "!pip install box2d_py\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  autotools-dev cmake-data file ibverbs-providers libarchive13 libfabric1\n",
            "  libhwloc-dev libhwloc-plugins libhwloc5 libibverbs-dev libibverbs1\n",
            "  libjsoncpp1 libltdl-dev libltdl7 liblzo2-2 libmagic-mgc libmagic1\n",
            "  libnl-3-200 libnl-route-3-200 libnuma-dev libnuma1 libopenmpi2 libpciaccess0\n",
            "  libpsm-infinipath1 librdmacm1 librhash0 libtool libuv1 ocl-icd-libopencl1\n",
            "  openmpi-bin openmpi-common swig3.0\n",
            "Suggested packages:\n",
            "  cmake-doc ninja-build lrzip libhwloc-contrib-plugins libtool-doc openmpi-doc\n",
            "  pciutils autoconf automaken gcj-jdk swig-doc swig-examples swig3.0-examples\n",
            "  swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  autotools-dev cmake cmake-data file ibverbs-providers libarchive13\n",
            "  libfabric1 libhwloc-dev libhwloc-plugins libhwloc5 libibverbs-dev\n",
            "  libibverbs1 libjsoncpp1 libltdl-dev libltdl7 liblzo2-2 libmagic-mgc\n",
            "  libmagic1 libnl-3-200 libnl-route-3-200 libnuma-dev libnuma1 libopenmpi-dev\n",
            "  libopenmpi2 libpciaccess0 libpsm-infinipath1 librdmacm1 librhash0 libtool\n",
            "  libuv1 ocl-icd-libopencl1 openmpi-bin openmpi-common swig swig3.0\n",
            "0 upgraded, 35 newly installed, 0 to remove and 5 not upgraded.\n",
            "Need to get 11.5 MB of archives.\n",
            "After this operation, 58.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblzo2-2 amd64 2.08-1.2 [48.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.1 [184 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.1 [68.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.1 [22.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnuma1 amd64 2.0.11-2.1 [21.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 autotools-dev all 20180224.1 [39.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 cmake-data all 3.10.2-1ubuntu2 [1,331 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libarchive13 amd64 3.2.2-3.1ubuntu0.1 [289 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjsoncpp1 amd64 1.7.4-3 [73.6 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 librhash0 amd64 1.3.6-2 [78.1 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libuv1 amd64 1.18.0-3 [64.4 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 cmake amd64 3.10.2-1ubuntu2 [3,138 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnl-3-200 amd64 3.2.29-0ubuntu3 [52.8 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnl-route-3-200 amd64 3.2.29-0ubuntu3 [146 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libibverbs1 amd64 17.1-1 [44.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 ibverbs-providers amd64 17.1-1 [160 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libpsm-infinipath1 amd64 3.3+20.604758e7-5 [174 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 librdmacm1 amd64 17.1-1 [56.1 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libfabric1 amd64 1.5.3-1 [302 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libltdl7 amd64 2.4.6-2 [38.8 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libltdl-dev amd64 2.4.6-2 [162 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpciaccess0 amd64 0.14-1 [17.9 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtool all 2.4.6-2 [194 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libhwloc5 amd64 1.11.9-1 [105 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 ocl-icd-libopencl1 amd64 2.2.11-1ubuntu1 [30.3 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libhwloc-plugins amd64 1.11.9-1 [12.5 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenmpi2 amd64 2.1.1-8 [2,056 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/universe amd64 openmpi-common all 2.1.1-8 [140 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/universe amd64 openmpi-bin amd64 2.1.1-8 [88.2 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnuma-dev amd64 2.0.11-2.1 [32.2 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libhwloc-dev amd64 1.11.9-1 [167 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/main amd64 libibverbs-dev amd64 17.1-1 [103 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenmpi-dev amd64 2.1.1-8 [925 kB]\n",
            "Fetched 11.5 MB in 2s (5,314 kB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "(Reading database ... 22280 files and directories currently installed.)\n",
            "Preparing to unpack .../00-liblzo2-2_2.08-1.2_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.08-1.2) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "Preparing to unpack .../01-libmagic-mgc_1%3a5.32-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../02-libmagic1_1%3a5.32-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.1) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../03-file_1%3a5.32-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libnuma1:amd64.\n",
            "Preparing to unpack .../04-libnuma1_2.0.11-2.1_amd64.deb ...\n",
            "Unpacking libnuma1:amd64 (2.0.11-2.1) ...\n",
            "Selecting previously unselected package autotools-dev.\n",
            "Preparing to unpack .../05-autotools-dev_20180224.1_all.deb ...\n",
            "Unpacking autotools-dev (20180224.1) ...\n",
            "Selecting previously unselected package cmake-data.\n",
            "Preparing to unpack .../06-cmake-data_3.10.2-1ubuntu2_all.deb ...\n",
            "Unpacking cmake-data (3.10.2-1ubuntu2) ...\n",
            "Selecting previously unselected package libarchive13:amd64.\n",
            "Preparing to unpack .../07-libarchive13_3.2.2-3.1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libarchive13:amd64 (3.2.2-3.1ubuntu0.1) ...\n",
            "Selecting previously unselected package libjsoncpp1:amd64.\n",
            "Preparing to unpack .../08-libjsoncpp1_1.7.4-3_amd64.deb ...\n",
            "Unpacking libjsoncpp1:amd64 (1.7.4-3) ...\n",
            "Selecting previously unselected package librhash0:amd64.\n",
            "Preparing to unpack .../09-librhash0_1.3.6-2_amd64.deb ...\n",
            "Unpacking librhash0:amd64 (1.3.6-2) ...\n",
            "Selecting previously unselected package libuv1:amd64.\n",
            "Preparing to unpack .../10-libuv1_1.18.0-3_amd64.deb ...\n",
            "Unpacking libuv1:amd64 (1.18.0-3) ...\n",
            "Selecting previously unselected package cmake.\n",
            "Preparing to unpack .../11-cmake_3.10.2-1ubuntu2_amd64.deb ...\n",
            "Unpacking cmake (3.10.2-1ubuntu2) ...\n",
            "Selecting previously unselected package libnl-3-200:amd64.\n",
            "Preparing to unpack .../12-libnl-3-200_3.2.29-0ubuntu3_amd64.deb ...\n",
            "Unpacking libnl-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
            "Selecting previously unselected package libnl-route-3-200:amd64.\n",
            "Preparing to unpack .../13-libnl-route-3-200_3.2.29-0ubuntu3_amd64.deb ...\n",
            "Unpacking libnl-route-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
            "Selecting previously unselected package libibverbs1:amd64.\n",
            "Preparing to unpack .../14-libibverbs1_17.1-1_amd64.deb ...\n",
            "Unpacking libibverbs1:amd64 (17.1-1) ...\n",
            "Selecting previously unselected package ibverbs-providers:amd64.\n",
            "Preparing to unpack .../15-ibverbs-providers_17.1-1_amd64.deb ...\n",
            "Unpacking ibverbs-providers:amd64 (17.1-1) ...\n",
            "Selecting previously unselected package libpsm-infinipath1.\n",
            "Preparing to unpack .../16-libpsm-infinipath1_3.3+20.604758e7-5_amd64.deb ...\n",
            "Unpacking libpsm-infinipath1 (3.3+20.604758e7-5) ...\n",
            "Selecting previously unselected package librdmacm1:amd64.\n",
            "Preparing to unpack .../17-librdmacm1_17.1-1_amd64.deb ...\n",
            "Unpacking librdmacm1:amd64 (17.1-1) ...\n",
            "Selecting previously unselected package libfabric1.\n",
            "Preparing to unpack .../18-libfabric1_1.5.3-1_amd64.deb ...\n",
            "Unpacking libfabric1 (1.5.3-1) ...\n",
            "Selecting previously unselected package libltdl7:amd64.\n",
            "Preparing to unpack .../19-libltdl7_2.4.6-2_amd64.deb ...\n",
            "Unpacking libltdl7:amd64 (2.4.6-2) ...\n",
            "Selecting previously unselected package libltdl-dev:amd64.\n",
            "Preparing to unpack .../20-libltdl-dev_2.4.6-2_amd64.deb ...\n",
            "Unpacking libltdl-dev:amd64 (2.4.6-2) ...\n",
            "Selecting previously unselected package libpciaccess0:amd64.\n",
            "Preparing to unpack .../21-libpciaccess0_0.14-1_amd64.deb ...\n",
            "Unpacking libpciaccess0:amd64 (0.14-1) ...\n",
            "Selecting previously unselected package libtool.\n",
            "Preparing to unpack .../22-libtool_2.4.6-2_all.deb ...\n",
            "Unpacking libtool (2.4.6-2) ...\n",
            "Selecting previously unselected package libhwloc5:amd64.\n",
            "Preparing to unpack .../23-libhwloc5_1.11.9-1_amd64.deb ...\n",
            "Unpacking libhwloc5:amd64 (1.11.9-1) ...\n",
            "Selecting previously unselected package ocl-icd-libopencl1:amd64.\n",
            "Preparing to unpack .../24-ocl-icd-libopencl1_2.2.11-1ubuntu1_amd64.deb ...\n",
            "Unpacking ocl-icd-libopencl1:amd64 (2.2.11-1ubuntu1) ...\n",
            "Selecting previously unselected package libhwloc-plugins.\n",
            "Preparing to unpack .../25-libhwloc-plugins_1.11.9-1_amd64.deb ...\n",
            "Unpacking libhwloc-plugins (1.11.9-1) ...\n",
            "Selecting previously unselected package libopenmpi2:amd64.\n",
            "Preparing to unpack .../26-libopenmpi2_2.1.1-8_amd64.deb ...\n",
            "Unpacking libopenmpi2:amd64 (2.1.1-8) ...\n",
            "Selecting previously unselected package openmpi-common.\n",
            "Preparing to unpack .../27-openmpi-common_2.1.1-8_all.deb ...\n",
            "Unpacking openmpi-common (2.1.1-8) ...\n",
            "Selecting previously unselected package openmpi-bin.\n",
            "Preparing to unpack .../28-openmpi-bin_2.1.1-8_amd64.deb ...\n",
            "Unpacking openmpi-bin (2.1.1-8) ...\n",
            "Selecting previously unselected package swig3.0.\n",
            "Preparing to unpack .../29-swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../30-swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Selecting previously unselected package libnuma-dev:amd64.\n",
            "Preparing to unpack .../31-libnuma-dev_2.0.11-2.1_amd64.deb ...\n",
            "Unpacking libnuma-dev:amd64 (2.0.11-2.1) ...\n",
            "Selecting previously unselected package libhwloc-dev:amd64.\n",
            "Preparing to unpack .../32-libhwloc-dev_1.11.9-1_amd64.deb ...\n",
            "Unpacking libhwloc-dev:amd64 (1.11.9-1) ...\n",
            "Selecting previously unselected package libibverbs-dev:amd64.\n",
            "Preparing to unpack .../33-libibverbs-dev_17.1-1_amd64.deb ...\n",
            "Unpacking libibverbs-dev:amd64 (17.1-1) ...\n",
            "Selecting previously unselected package libopenmpi-dev.\n",
            "Preparing to unpack .../34-libopenmpi-dev_2.1.1-8_amd64.deb ...\n",
            "Unpacking libopenmpi-dev (2.1.1-8) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up libuv1:amd64 (1.18.0-3) ...\n",
            "Setting up libnuma1:amd64 (2.0.11-2.1) ...\n",
            "Setting up cmake-data (3.10.2-1ubuntu2) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.1) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.1) ...\n",
            "Setting up librhash0:amd64 (1.3.6-2) ...\n",
            "Setting up libpsm-infinipath1 (3.3+20.604758e7-5) ...\n",
            "update-alternatives: using /usr/lib/libpsm1/libpsm_infinipath.so.1.16 to provide /usr/lib/x86_64-linux-gnu/libpsm_infinipath.so.1 (libpsm_infinipath.so.1) in auto mode\n",
            "Setting up openmpi-common (2.1.1-8) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up autotools-dev (20180224.1) ...\n",
            "Setting up libltdl7:amd64 (2.4.6-2) ...\n",
            "Setting up libpciaccess0:amd64 (0.14-1) ...\n",
            "Setting up libnuma-dev:amd64 (2.0.11-2.1) ...\n",
            "Setting up ocl-icd-libopencl1:amd64 (2.2.11-1ubuntu1) ...\n",
            "Setting up libnl-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
            "Setting up liblzo2-2:amd64 (2.08-1.2) ...\n",
            "Setting up libjsoncpp1:amd64 (1.7.4-3) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Setting up libltdl-dev:amd64 (2.4.6-2) ...\n",
            "Setting up libarchive13:amd64 (3.2.2-3.1ubuntu0.1) ...\n",
            "Setting up libnl-route-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
            "Setting up file (1:5.32-2ubuntu0.1) ...\n",
            "Setting up libhwloc5:amd64 (1.11.9-1) ...\n",
            "Setting up libhwloc-plugins (1.11.9-1) ...\n",
            "Setting up cmake (3.10.2-1ubuntu2) ...\n",
            "Setting up libtool (2.4.6-2) ...\n",
            "Setting up libibverbs1:amd64 (17.1-1) ...\n",
            "Setting up libhwloc-dev:amd64 (1.11.9-1) ...\n",
            "Setting up librdmacm1:amd64 (17.1-1) ...\n",
            "Setting up ibverbs-providers:amd64 (17.1-1) ...\n",
            "Setting up libibverbs-dev:amd64 (17.1-1) ...\n",
            "Setting up libfabric1 (1.5.3-1) ...\n",
            "Setting up libopenmpi2:amd64 (2.1.1-8) ...\n",
            "Setting up libopenmpi-dev (2.1.1-8) ...\n",
            "update-alternatives: using /usr/lib/x86_64-linux-gnu/openmpi/include to provide /usr/include/mpi (mpi) in auto mode\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpicc.1.gz because associated file /usr/share/man/man1/mpicc.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpic++.1.gz because associated file /usr/share/man/man1/mpic++.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpicxx.1.gz because associated file /usr/share/man/man1/mpicxx.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpiCC.1.gz because associated file /usr/share/man/man1/mpiCC.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpif77.1.gz because associated file /usr/share/man/man1/mpif77.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpif90.1.gz because associated file /usr/share/man/man1/mpif90.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpifort.1.gz because associated file /usr/share/man/man1/mpifort.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "Setting up openmpi-bin (2.1.1-8) ...\n",
            "update-alternatives: using /usr/bin/mpirun.openmpi to provide /usr/bin/mpirun (mpirun) in auto mode\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpirun.1.gz because associated file /usr/share/man/man1/mpirun.openmpi.1.gz (of link group mpirun) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpiexec.1.gz because associated file /usr/share/man/man1/mpiexec.openmpi.1.gz (of link group mpirun) doesn't exist\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Collecting gym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/22/4ff09745ade385ffe707fb5f053548f0f6a6e7d5e98a2b9d6c07f5b931a7/gym-0.10.9.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Collecting pyglet>=1.2.0 (from gym)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.10.15)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6c/3a/0e/b86dee98876bb56cdb482cc1f72201035e46d1baf69d10d028\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "Successfully installed gym-0.10.9 pyglet-1.3.2\n",
            "Collecting box2d_py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/71/f7cdeddf91dcc976448678d2e4b84b83ce868281e97b3ccbbfa685d120e0/box2d_py-2.3.6-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.1MB 8.3MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jAJVYB0fmsly"
      },
      "cell_type": "markdown",
      "source": [
        "# Check if we are allocated a GPU\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dlFeN7DQDovH",
        "outputId": "e94e854b-3d9b-45bb-b5d2-2c4390fb1e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lN5wW0Y18oMR"
      },
      "cell_type": "markdown",
      "source": [
        "# Connect to Google Drive"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1W29us8L6-Go",
        "outputId": "3ba6f96b-17c8-4bfb-8a00-7fefec8261f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2AooYTa76TDn"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Deterministic Policy Gradient\n",
        "\n",
        "In this implementation, the actions of the BipedalWalker are discretized into 81 actions, each action being a permutation of {-1,0,1} for each of the four outputs."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UTrPpfVa6TDr"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Modules"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zIQ2JJ_p6TDv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import gym\n",
        "import os\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, BatchNormalization, Input, Add, LeakyReLU, Subtract, Concatenate, Lambda\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "import pickle # for saving episodes -> rewards\n",
        "\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from tqdm import tqdm, trange, tnrange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cWVPsYUt6TD8"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the Model"
      ]
    },
    {
      "metadata": {
        "id": "F150YH8nTBwj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create Noise"
      ]
    },
    {
      "metadata": {
        "id": "0_iIHWpCTBwk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py, which is\n",
        "# based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
        "class OrnsteinUhlenbeckActionNoise:\n",
        "    def __init__(self, mu, sigma=0.5, theta=.20, dt=1e-2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TCOvLXLK6TD-"
      },
      "cell_type": "markdown",
      "source": [
        "### Replay Buffer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Nx5ibJ9n6TEA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    This class represents the experience replay buffer\n",
        "    \"\"\"\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "        self.capacity = buffer_size\n",
        "        self.len = 0\n",
        "    \n",
        "    def sample(self, n_samples):\n",
        "        batch = []\n",
        "        n_samples = min(self.len, n_samples)\n",
        "        batch = random.sample(self.buffer, n_samples)\n",
        "        \n",
        "        curr_states = np.float32([arr[0] for arr in batch])\n",
        "        actions = np.int32([arr[1] for arr in batch])\n",
        "        rewards = np.float32([arr[2] for arr in batch])\n",
        "        done = np.bool_([arr[3] for arr in batch])\n",
        "        next_states = np.float32([arr[4] for arr in batch])\n",
        "        \n",
        "        return np.array(curr_states), np.array(actions), np.array(rewards), np.array(done), np.array(next_states)\n",
        "    \n",
        "    def add(self, curr_state, action, reward, done, next_state):\n",
        "        self.buffer.append([curr_state, action, reward, done, next_state])\n",
        "        self.len = self.len + 1\n",
        "        if (self.len > self.capacity):\n",
        "            self.len = self.capacity\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9L59sqSVTBwr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Actor Network"
      ]
    },
    {
      "metadata": {
        "id": "_pDbVi2mTBws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Actor():\n",
        "    \"\"\"\n",
        "    Input to the network is the state, output is the action\n",
        "    under a deterministic policy.\n",
        "    The output layer activation is a tanh to keep the action\n",
        "    between -action_bound and action_bound\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, learning_rate, tau):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.tau = tau  # tau is the update rate for the target network - allowing it to slowly chase\n",
        "        \n",
        "        # Actor Network\n",
        "        self.model = self.create_actor_network()\n",
        "        self.actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
        "        \n",
        "        print(self.model)\n",
        "        # Target Network\n",
        "        self.target = self.create_actor_network()\n",
        "\n",
        "    def create_actor_network(self):\n",
        "        model = Sequential()\n",
        "        #print(model)\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dense(256, input_dim=self.state_dim, activation='linear'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        #print(model)\n",
        "        model.add(Dense(256, activation='linear'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        #print(model)\n",
        "        model.add(Dense(256, activation='linear'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        #print(model)\n",
        "        # output of bipedal walker is between -1 and 1\n",
        "        model.add(Dense(self.action_dim, activation='tanh'))\n",
        "        #print(model)\n",
        "        model.compile(\n",
        "            optimizer=Adam(lr=self.learning_rate, ),\n",
        "            loss=\"mse\"\n",
        "        )\n",
        "        #model.summary()\n",
        "        print(model)\n",
        "        \n",
        "        return model\n",
        "\n",
        "    def fit(self, states, action_gradients):\n",
        "        self.model.fit(states, action_gradients, epochs=5, verbose=0)\n",
        "    \n",
        "    def noisy_predict(self, inputs):\n",
        "        return self.model.predict(inputs, verbose=0) + self.actor_noise() \n",
        "    \n",
        "    def predict(self, states):\n",
        "        return self.model.predict(states, verbose=0)\n",
        "\n",
        "    def predict_target(self, states):\n",
        "        return self.target.predict(states, verbose=0)\n",
        "    \n",
        "    def update_target(self):\n",
        "        \"\"\"\n",
        "        Soft update of target network\n",
        "        \"\"\"\n",
        "        self.target.set_weights(\n",
        "            [x[0] * self.tau + x[1] * (1 - self.tau) for x in zip(self.model.get_weights(), self.target.get_weights())]\n",
        "        )\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qauv6GHB6TEJ"
      },
      "cell_type": "markdown",
      "source": [
        "## Critic Network"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VadZr9_N6TEL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Critic():\n",
        "    \"\"\"\n",
        "    This network takes in 2 inputs the state and action, \n",
        "    the output is the Q(s,a)\n",
        "    The input action is given by the Actor\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, sess, state_dim, action_dim, \n",
        "                 learning_rate, tau, gamma):\n",
        "        self.sess = sess\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.tau = tau\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # Session needed to grab action gradients\n",
        "        K.set_session(sess)\n",
        "\n",
        "        # Create networks\n",
        "        self.model, self.state, self.action = self.create_critic_network()\n",
        "        self.target, self.target_action, self.target_state = self.create_critic_network()\n",
        "        \n",
        "        self.action_grads = tf.gradients(self.model.output, self.action)\n",
        "        self.sess.run(tf.initialize_all_variables())\n",
        "\n",
        "    def create_critic_network(self):\n",
        "        state_inputs = Input(shape=(self.state_dim,), name='state_input')\n",
        "        action_inputs = Input(shape=(self.action_dim,), name='action_input')\n",
        "        \n",
        "        state_net = BatchNormalization()(state_inputs)\n",
        "        \n",
        "        state_net = Dense(256)(state_net)\n",
        "        state_net = BatchNormalization()(state_net)\n",
        "        state_net = LeakyReLU(alpha=0.2)(state_net)\n",
        "        \n",
        "        state_net = Dense(256)(state_net)\n",
        "        state_net = BatchNormalization()(state_net)\n",
        "        state_net = LeakyReLU(alpha=0.2)(state_net)\n",
        "        \n",
        "        state_net = Dense(256)(state_net)\n",
        "        state_net = BatchNormalization()(state_net)\n",
        "        state_net = LeakyReLU(alpha=0.2)(state_net)\n",
        "        state_net = Dense(300)(state_net)\n",
        "        \n",
        "        # Actions do not need to be normalized - already between -1 and 1\n",
        "        action_net = Dense(300)(action_inputs)\n",
        "        \n",
        "        # Combine state_net and action_net\n",
        "        net = Add()([state_net, action_net])\n",
        "        net = Dense(300)(net)\n",
        "        net = BatchNormalization()(net)\n",
        "        net = LeakyReLU(alpha=0.2)(net)\n",
        "        net = Dense(1)(net)\n",
        "        \n",
        "        model = keras.models.Model(inputs=[state_inputs, action_inputs], outputs=net)\n",
        "        model.compile(\n",
        "            optimizer=Adam(lr=self.learning_rate),\n",
        "            loss=\"mse\"\n",
        "        )\n",
        "        model.summary()\n",
        "        \n",
        "        return model, state_inputs, action_inputs\n",
        "\n",
        "    def fit(self, states, actions, predicted_q_values):\n",
        "        self.model.fit({'state_input': states, 'action_input': actions}, predicted_q_values, verbose=0)\n",
        "\n",
        "    def predict(self, states, action):\n",
        "        return self.model.predict({'state_input': states, 'action_input': actions}, verbose=0)\n",
        "\n",
        "    def predict_target(self, states, actions):\n",
        "        return self.target.predict({'state_input': states, 'action_input': actions}, verbose=0)\n",
        "    \n",
        "    def get_action_gradients(self, states, actions):\n",
        "        return self.sess.run(\n",
        "            self.action_grads,\n",
        "            feed_dict={\n",
        "                self.state: states,\n",
        "                self.action: actions\n",
        "            })[0]\n",
        "    \n",
        "    def update_target(self):\n",
        "        self.target.set_weights(\n",
        "            [x[0] * self.tau + x[1] * (1 - self.tau) for x in zip(self.model.get_weights(), self.target.get_weights())]\n",
        "        )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8_qZBkkTBFyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Intrinsic Curiosity Module"
      ]
    },
    {
      "metadata": {
        "id": "riVCRXMLApNA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ICM:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim        \n",
        "        self.state_encoder, self.trainer, self.predictor = self.create_models()\n",
        "    \n",
        "    def create_models(self):\n",
        "        feature_dim = 32\n",
        "        state_input = Input(shape=(self.state_dim,), name='state_input')\n",
        "        next_state_input = Input(shape=(self.state_dim,), name='next_state_input')\n",
        "        action_input = Input(shape=(self.action_dim,), name='action_input')\n",
        "        \n",
        "        # Feature Encoder\n",
        "        state_encoder_input = Input(shape=(self.state_dim,))\n",
        "        state_encoder = Dense(128)(state_encoder_input)\n",
        "        state_encoder = Dense(128)(state_encoder)\n",
        "        encoded_state = Dense(feature_dim)(state_encoder)\n",
        "        state_encoder_model = Model(input=state_encoder_input, output=encoded_state)\n",
        "        \n",
        "        encoded_state = state_encoder_model(state_input)\n",
        "        encoded_next_state = state_encoder_model(next_state_input)\n",
        "        \n",
        "        # Inverse Model\n",
        "        inverse_model = Concatenate()([encoded_state, encoded_next_state])\n",
        "        inverse_model = Dense(128)(inverse_model)\n",
        "        inverse_model = Dense(128)(inverse_model)\n",
        "        action_output = Dense(self.action_dim, name='action_output')(inverse_model)\n",
        "        \n",
        "        # Forward Model\n",
        "        forward_model = Concatenate()([action_input, encoded_state])\n",
        "        forward_model = Dense(128)(forward_model)\n",
        "        forward_model = Dense(128)(forward_model)\n",
        "        next_state_output = Dense(feature_dim, name='next_state_output')(forward_model)\n",
        "        \n",
        "        # Reward Output\n",
        "        reward_processor = Subtract()([next_state_output, encoded_next_state])\n",
        "        reward_output = Lambda((lambda x: K.sum(x ** 2)), name='reward_output')(reward_processor)\n",
        "        \n",
        "        # All combined\n",
        "        trainer = Model(input=[action_input, state_input, next_state_input], output=[next_state_output, action_output])\n",
        "        trainer.compile(optimizer='adam', loss='mse')\n",
        "        predictor = Model(input=[action_input, state_input, next_state_input], output=reward_output)\n",
        "        \n",
        "        return state_encoder_model, trainer, predictor\n",
        "    \n",
        "    def fit(self, states, actions, next_states):\n",
        "        encoded_states = self.state_encoder.predict(states)\n",
        "        self.trainer.fit({\n",
        "            'state_input': states,\n",
        "            'next_state_input': next_states,\n",
        "            'action_input': actions,\n",
        "        },{\n",
        "            'next_state_output': encoded_states,\n",
        "            'action_output': actions,\n",
        "        }, verbose=0)\n",
        "        \n",
        "    def predict(self, states, actions, next_states):\n",
        "        return self.predictor.predict({\n",
        "            'state_input': states,\n",
        "            'next_state_input': next_states,\n",
        "            'action_input': actions,\n",
        "        }, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sp0qXL3ID7NW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "g-MSGXm06TES"
      },
      "cell_type": "markdown",
      "source": [
        "## Create the Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S6tv0vTh6TEU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DDPGAgent:\n",
        "    def __init__(self, sess, state_dim, action_dim, buffer_size=30000, \n",
        "                 learning_rate=0.001, batch_size=64, gamma=0.9, \n",
        "                 epsilon=1.00, epsilon_decay=0.99999, epsilon_min=0.001,\n",
        "                 name='DDPG', tau=0.001):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.tau = tau\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        \n",
        "        self.name = name;\n",
        "        \n",
        "        self.actor = Actor(state_dim, action_dim, learning_rate, tau)\n",
        "        self.critic = Critic(sess, state_dim, action_dim, learning_rate, tau, gamma)\n",
        "        self.icm = ICM(state_dim, action_dim)\n",
        "        \n",
        "        self.buffer = ReplayBuffer(buffer_size)\n",
        "        \n",
        "    \n",
        "    def get_noisy_action(self, states):\n",
        "        states = np.reshape(states, (1, self.state_dim))\n",
        "        return self.actor.noisy_predict(states)\n",
        "    \n",
        "    \n",
        "    def train_model(self):     \n",
        "        if (self.buffer.len > self.batch_size):\n",
        "            states, actions, rewards, done, next_states = self.buffer.sample(self.batch_size)\n",
        "            self.icm.fit(states, actions, next_states)\n",
        "            rewards_i = self.icm.predict(states, actions, next_states)\n",
        "            target_q = self.critic.predict_target(next_states, actions)\n",
        "            \n",
        "            targets = []\n",
        "            for i in range(self.batch_size):\n",
        "                if (done[i]):\n",
        "                    targets.append(rewards[i] + rewards_i[i])\n",
        "                else:\n",
        "                    targets.append(rewards[i] + rewards_i[i] + self.gamma + target_q[i])\n",
        "            \n",
        "            # Update critic\n",
        "            self.critic.fit(states, actions, np.reshape(targets, (-1,1)))\n",
        "            \n",
        "            actor_actions = self.actor.predict(states)\n",
        "            action_gradients = self.critic.get_action_gradients(states, actor_actions)\n",
        "            self.actor.fit(states, action_gradients)\n",
        "            \n",
        "            self.actor.update_target()\n",
        "            self.critic.update_target()\n",
        "    \n",
        "    def store_transition(self, state, action, reward, done, next_state):\n",
        "        self.buffer.add(state, action, reward, done, next_state)\n",
        "    \n",
        "    def save_model(self, n_episodes):\n",
        "        GOOGLE_DIR = '/content/gdrive/My Drive/cs4246_project/models/ddpg/trained_models/'\n",
        "        HOME_DIR = './'\n",
        "        self.actor.model.save(HOME_DIR + 'actor' + '_ep' + str(n_episodes) + '.h5')\n",
        "        self.critic.model.save(HOME_DIR + 'critic' + '_ep' + str(n_episodes) + '.h5')\n",
        "        pass\n",
        "    \n",
        "    def load_model(self, model_name):\n",
        "        self.model = keras.models.load_model(model_name)\n",
        "        pass\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gHEuO73V6TEd"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup Gym Environment and Initialize Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JVp6RZN76TEg",
        "outputId": "d1c7bae2-83d6-412a-cb4a-cb3dd2631027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1771
        }
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('BipedalWalker-v2')\n",
        "n_state_params = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.shape[0]\n",
        "\n",
        "# allow GPU optimization\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.allow_growth = True\n",
        "#sess = tf.Session(config=config)\n",
        "sess = tf.Session()\n",
        "\n",
        "agent = DDPGAgent(sess, n_state_params, n_actions)\n",
        "BATCH_SIZE = 64\n",
        "MAX_EPISODES = 100000\n",
        "MAX_REWARD = 300\n",
        "MAX_STEPS = env._max_episode_steps\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "<keras.engine.sequential.Sequential object at 0x7f60747c8358>\n",
            "<keras.engine.sequential.Sequential object at 0x7f60747c8358>\n",
            "<keras.engine.sequential.Sequential object at 0x7f60747c8cf8>\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "state_input (InputLayer)        (None, 24)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 24)           96          state_input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_266 (Dense)               (None, 256)          6400        batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 256)          1024        dense_266[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_147 (LeakyReLU)     (None, 256)          0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_267 (Dense)               (None, 256)          65792       leaky_re_lu_147[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 256)          1024        dense_267[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_148 (LeakyReLU)     (None, 256)          0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_268 (Dense)               (None, 256)          65792       leaky_re_lu_148[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 256)          1024        dense_268[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_149 (LeakyReLU)     (None, 256)          0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "action_input (InputLayer)       (None, 4)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_269 (Dense)               (None, 300)          77100       leaky_re_lu_149[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_270 (Dense)               (None, 300)          1500        action_input[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 300)          0           dense_269[0][0]                  \n",
            "                                                                 dense_270[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_271 (Dense)               (None, 300)          90300       add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 300)          1200        dense_271[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_150 (LeakyReLU)     (None, 300)          0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_272 (Dense)               (None, 1)            301         leaky_re_lu_150[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 311,553\n",
            "Trainable params: 309,369\n",
            "Non-trainable params: 2,184\n",
            "__________________________________________________________________________________________________\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "state_input (InputLayer)        (None, 24)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 24)           96          state_input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_273 (Dense)               (None, 256)          6400        batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 256)          1024        dense_273[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_151 (LeakyReLU)     (None, 256)          0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_274 (Dense)               (None, 256)          65792       leaky_re_lu_151[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 256)          1024        dense_274[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_152 (LeakyReLU)     (None, 256)          0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_275 (Dense)               (None, 256)          65792       leaky_re_lu_152[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 256)          1024        dense_275[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_153 (LeakyReLU)     (None, 256)          0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "action_input (InputLayer)       (None, 4)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_276 (Dense)               (None, 300)          77100       leaky_re_lu_153[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_277 (Dense)               (None, 300)          1500        action_input[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 300)          0           dense_276[0][0]                  \n",
            "                                                                 dense_277[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_278 (Dense)               (None, 300)          90300       add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 300)          1200        dense_278[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_154 (LeakyReLU)     (None, 300)          0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_279 (Dense)               (None, 1)            301         leaky_re_lu_154[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 311,553\n",
            "Trainable params: 309,369\n",
            "Non-trainable params: 2,184\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"re...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nRZVGKeT6TEu"
      },
      "cell_type": "markdown",
      "source": [
        "## Run Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dU6wltrp6TEv",
        "outputId": "a847e66a-bcbd-4992-c970-68558b51595d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18224
        }
      },
      "cell_type": "code",
      "source": [
        "for ep in range(MAX_EPISODES):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    for t in range(MAX_STEPS):\n",
        "        prog = tf.keras.utils.Progbar(MAX_STEPS)\n",
        "        state = np.reshape(state, [1, n_state_params])\n",
        "        actions = agent.get_noisy_action(state)\n",
        "        state = np.reshape(state, [n_state_params])\n",
        "        next_state, reward, isDone, _ = env.step(actions[0])\n",
        "        \n",
        "        agent.store_transition(state, actions[0], reward, isDone, next_state)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        prog.update(t)\n",
        "        if (isDone):\n",
        "            prog.update(MAX_STEPS)\n",
        "            print(\"episode: {}/{}, score: {}, e: {:.2}\".format(ep, MAX_EPISODES, total_reward, agent.epsilon))\n",
        "            break\n",
        "    if (agent.buffer.len > BATCH_SIZE):\n",
        "        agent.train_model()\n",
        "    \n",
        "    # record rewards dynamically\n",
        "    GOOGLE_FILE = '/content/gdrive/My Drive/record.dat'\n",
        "    HOME_FILE = './record.dat'\n",
        "    record_filename = GOOGLE_FILE\n",
        "    data = [ep, total_reward]\n",
        "    with open(record_filename, \"ab\") as f:\n",
        "        pickle.dump(data, f)\n",
        "    \n",
        "    if (total_reward > 200):\n",
        "        agent.save_model(ep)\n",
        "        break\n",
        "    \n",
        "    # save model every 10000 episodes\n",
        "    if ((ep % 10000) == 0):\n",
        "        agent.save_model(ep)\n",
        "    \n",
        "    if (total_reward > 200):\n",
        "        agent.save_model(ep)\n",
        "        break\n",
        "    \n",
        "    # save model every 100 episodes\n",
        "    if ((ep % 100) == 0):\n",
        "        agent.save_model(ep)\n",
        "        \n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 0/100000, score: -101.02764031297515, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 1/100000, score: -98.43909253419848, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 2/100000, score: -118.87385683593364, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 3/100000, score: -132.59563647794818, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 4/100000, score: -111.25222020680071, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 5/100000, score: -129.37717455128353, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 6/100000, score: -129.45024779963697, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 7/100000, score: -112.57614215879774, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 8/100000, score: -134.00041068707057, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 9/100000, score: -109.49361843645134, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 10/100000, score: -111.88126622490496, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 11/100000, score: -112.8188575200161, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 12/100000, score: -152.66884828497433, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 13/100000, score: -100.02996818271781, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 14/100000, score: -98.46894151914535, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 15/100000, score: -101.81608172539879, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 16/100000, score: -103.52328051606153, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 17/100000, score: -102.35437573552902, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 18/100000, score: -105.49824641224751, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 19/100000, score: -103.77160271404159, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 20/100000, score: -106.48925366721733, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 21/100000, score: -106.3531974378977, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 22/100000, score: -108.30576379467286, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 23/100000, score: -104.92074823102783, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 24/100000, score: -106.3815068178114, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 25/100000, score: -110.32102829012422, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 26/100000, score: -105.28791884724117, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 27/100000, score: -97.97002199685996, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 28/100000, score: -130.25489740325784, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 29/100000, score: -108.19723102072041, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 30/100000, score: -125.91673196243104, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 31/100000, score: -129.50401746032588, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 32/100000, score: -126.57041712718708, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 33/100000, score: -127.12570732693848, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 34/100000, score: -127.57284167984477, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 35/100000, score: -138.0421741240382, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 36/100000, score: -99.87090783534337, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 37/100000, score: -97.44430664916567, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 38/100000, score: -100.27641836292523, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 39/100000, score: -99.9476155139943, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 40/100000, score: -99.01082904387603, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 41/100000, score: -99.0780575077289, e: 1.0\n",
            "1600/1600 [==============================] - 0s 12us/step\n",
            "episode: 42/100000, score: -98.69169857054774, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 43/100000, score: -100.81838031183582, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 44/100000, score: -121.12657312166233, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 45/100000, score: -100.38935470466814, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 46/100000, score: -104.82731348106097, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 47/100000, score: -99.78082358325067, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 48/100000, score: -101.03706708897441, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 49/100000, score: -98.69235358323944, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 50/100000, score: -99.53475917988524, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 51/100000, score: -125.02203498834733, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 52/100000, score: -151.33076360466453, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 53/100000, score: -101.63371200666013, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 54/100000, score: -99.44307890712375, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 55/100000, score: -98.18089047908249, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 56/100000, score: -106.67768984155387, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 57/100000, score: -100.4923520800431, e: 1.0\n",
            "1600/1600 [==============================] - 0s 7us/step\n",
            "episode: 58/100000, score: -108.97966257209738, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 59/100000, score: -130.21446715962367, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 60/100000, score: -96.44564599794624, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 61/100000, score: -111.16672233269082, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 62/100000, score: -105.841432646251, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 63/100000, score: -100.46667070869673, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 64/100000, score: -101.06920769172628, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 65/100000, score: -102.28434179415626, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 66/100000, score: -101.66209879681738, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 67/100000, score: -105.78612293318925, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 68/100000, score: -100.334173741928, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 69/100000, score: -106.4235455234561, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 70/100000, score: -99.89407658063146, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 71/100000, score: -130.10348929226998, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 72/100000, score: -101.60132384504156, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 73/100000, score: -101.43590232227594, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 74/100000, score: -100.88079647174544, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 75/100000, score: -103.15774837698851, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 76/100000, score: -102.00972553802188, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 77/100000, score: -100.71625519825382, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 78/100000, score: -98.25601766224032, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 79/100000, score: -99.84917166380458, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 80/100000, score: -101.49354570675607, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 81/100000, score: -100.57995423057834, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 82/100000, score: -98.82105561444574, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 83/100000, score: -99.25549409397811, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 84/100000, score: -104.33132705818292, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 85/100000, score: -101.89626877164247, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 86/100000, score: -102.23960430743318, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 87/100000, score: -98.7698092158351, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 88/100000, score: -99.1836899982358, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 89/100000, score: -100.42027565602, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 90/100000, score: -176.593258162906, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 91/100000, score: -112.92138225744412, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 92/100000, score: -114.37005242796239, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 93/100000, score: -120.42190892800343, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 94/100000, score: -150.5629040925128, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 95/100000, score: -110.7037997131642, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 96/100000, score: -114.9624970440384, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 97/100000, score: -106.5249169430668, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 98/100000, score: -111.834783926242, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 99/100000, score: -112.15587897587697, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 100/100000, score: -122.35883663831589, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 101/100000, score: -105.69100092132881, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 102/100000, score: -112.3508148222068, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 103/100000, score: -112.17101785113424, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 104/100000, score: -123.71076452838345, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 105/100000, score: -125.88098513411268, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 106/100000, score: -119.82035800610637, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 107/100000, score: -121.4572823038568, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 108/100000, score: -113.20030032245994, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 109/100000, score: -111.77566256407678, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 110/100000, score: -115.00037292090728, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 111/100000, score: -112.35205503871981, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 112/100000, score: -117.09193338605681, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 113/100000, score: -107.77481816762287, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 114/100000, score: -99.1795560355963, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 115/100000, score: -126.43561256845618, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 116/100000, score: -113.94774994694711, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 117/100000, score: -105.30098773467114, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 118/100000, score: -183.05667368476955, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 119/100000, score: -104.82423859642451, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 120/100000, score: -102.21961148710027, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 121/100000, score: -99.83505277154654, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 122/100000, score: -103.15506788322169, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 123/100000, score: -99.9682201486767, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 124/100000, score: -99.79668692465006, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 125/100000, score: -99.67823666837788, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 126/100000, score: -106.64088340805003, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 127/100000, score: -100.05179310058018, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 128/100000, score: -99.08293932221764, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 129/100000, score: -99.91378339662845, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 130/100000, score: -100.88105257570164, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 131/100000, score: -101.14822106069853, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 132/100000, score: -102.30101455093349, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 133/100000, score: -101.91282298322318, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 134/100000, score: -100.4654843976992, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 135/100000, score: -190.96649436667394, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 136/100000, score: -107.63153277706344, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 137/100000, score: -107.04117015966509, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 138/100000, score: -99.64011721299948, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 139/100000, score: -127.03749304255791, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 140/100000, score: -98.63808328556738, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 141/100000, score: -127.01755882202932, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 142/100000, score: -106.5406849902916, e: 1.0\n",
            "1600/1600 [==============================] - 0s 8us/step\n",
            "episode: 143/100000, score: -102.05717165543771, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 144/100000, score: -108.00471489007188, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 145/100000, score: -105.87696301485096, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 146/100000, score: -100.24566662223104, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 147/100000, score: -208.16384430581812, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 148/100000, score: -100.91308730349698, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 149/100000, score: -100.53677757891408, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 150/100000, score: -99.21899731957407, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 151/100000, score: -99.8140073494075, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 152/100000, score: -99.2672085638963, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 153/100000, score: -101.13987247118823, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 154/100000, score: -97.43236977195, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 155/100000, score: -97.620969098416, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 156/100000, score: -102.73231562939972, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 157/100000, score: -96.97634219748406, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 158/100000, score: -97.83905477971436, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 159/100000, score: -99.88114367499652, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 160/100000, score: -100.00812219817252, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 161/100000, score: -99.56805676449437, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 162/100000, score: -99.21401966482854, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 163/100000, score: -98.84262708697727, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 164/100000, score: -101.34880299539533, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 165/100000, score: -151.2051508993206, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 166/100000, score: -119.36387698241262, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 167/100000, score: -109.32454114043742, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 168/100000, score: -118.42719807748068, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 169/100000, score: -113.70208971843691, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 170/100000, score: -116.19572212134962, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 171/100000, score: -110.34090752098133, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 172/100000, score: -112.81404422704944, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 173/100000, score: -114.0735800821272, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 174/100000, score: -117.31464464684063, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 175/100000, score: -126.89279621847606, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 176/100000, score: -204.6846653285998, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 177/100000, score: -100.68535306729395, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 178/100000, score: -97.29386936956024, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 179/100000, score: -103.7076321670323, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 180/100000, score: -121.90503016613481, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 181/100000, score: -103.0114476961795, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 182/100000, score: -150.99788948764188, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 183/100000, score: -126.22734830951225, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 184/100000, score: -112.04409654895485, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 185/100000, score: -137.56364108456748, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 186/100000, score: -194.23336944271927, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 187/100000, score: -115.94008081160692, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 188/100000, score: -115.39661558426457, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 189/100000, score: -115.65367221229415, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 190/100000, score: -113.92959009537576, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 191/100000, score: -110.0284057783258, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 192/100000, score: -113.66759807884404, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 193/100000, score: -116.82798527408008, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 194/100000, score: -129.1802744263822, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 195/100000, score: -117.99973285704124, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 196/100000, score: -122.2436367545983, e: 1.0\n",
            "1600/1600 [==============================] - 0s 8us/step\n",
            "episode: 197/100000, score: -126.00842036271325, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 198/100000, score: -120.86425618965356, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 199/100000, score: -110.87747141818443, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 200/100000, score: -116.77913304504673, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 201/100000, score: -110.78073770988999, e: 1.0\n",
            "1600/1600 [==============================] - 0s 7us/step\n",
            "episode: 202/100000, score: -103.68691761750964, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 203/100000, score: -132.2344553156595, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 204/100000, score: -111.6012755000676, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 205/100000, score: -120.82557021113371, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 206/100000, score: -118.18379154639169, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 207/100000, score: -120.03780243312661, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 208/100000, score: -129.26016058663072, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 209/100000, score: -121.31321043716184, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 210/100000, score: -111.05004254098682, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 211/100000, score: -118.78867979611043, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 212/100000, score: -107.82236917116967, e: 1.0\n",
            "1600/1600 [==============================] - 0s 7us/step\n",
            "episode: 213/100000, score: -115.4622232698918, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 214/100000, score: -115.31618208544084, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 215/100000, score: -113.03063187526861, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 216/100000, score: -118.02508544062493, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 217/100000, score: -115.10832840553971, e: 1.0\n",
            "1600/1600 [==============================] - 0s 7us/step\n",
            "episode: 218/100000, score: -117.57790879464699, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 219/100000, score: -110.36456213903953, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 220/100000, score: -138.44087935508406, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 221/100000, score: -97.91634882921417, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 222/100000, score: -104.21621173999782, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 223/100000, score: -97.42504083851763, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 224/100000, score: -113.4373366813901, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 225/100000, score: -98.78482815714895, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 226/100000, score: -98.52296496778726, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 227/100000, score: -96.02268945247847, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 228/100000, score: -104.72033149101178, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 229/100000, score: -176.44442143739997, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 230/100000, score: -122.60783112171227, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 231/100000, score: -143.42974342774306, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 232/100000, score: -131.8741374661856, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 233/100000, score: -165.7980534788117, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 234/100000, score: -116.2600469081578, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 235/100000, score: -110.0852656869203, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 236/100000, score: -224.95190194383918, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 237/100000, score: -106.29682862424116, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 238/100000, score: -101.6827300169903, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 239/100000, score: -101.39629741707209, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 240/100000, score: -100.1949508570644, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 241/100000, score: -100.90363832982553, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 242/100000, score: -101.32712330365445, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 243/100000, score: -100.43221922781287, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 244/100000, score: -100.31634354842596, e: 1.0\n",
            "1600/1600 [==============================] - 0s 7us/step\n",
            "episode: 245/100000, score: -100.97331235173283, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 246/100000, score: -104.71837249058778, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 247/100000, score: -98.25616036339633, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 248/100000, score: -98.40001333413721, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 249/100000, score: -101.40887296056565, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 250/100000, score: -100.91955047584383, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 251/100000, score: -104.69653310003063, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 252/100000, score: -99.63304041396916, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 253/100000, score: -106.71171586869636, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 254/100000, score: -103.1124445237898, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 255/100000, score: -98.15812304569641, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 256/100000, score: -104.91429478672354, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 257/100000, score: -135.27235609344007, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 258/100000, score: -113.37515999698054, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 259/100000, score: -107.29695720259194, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 260/100000, score: -107.7934777232423, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 261/100000, score: -98.22439087838013, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 262/100000, score: -110.9132522680808, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 263/100000, score: -119.21810658124372, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 264/100000, score: -102.22692448909699, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 265/100000, score: -132.94521470577425, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 266/100000, score: -106.63862042876241, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 267/100000, score: -99.61018096962098, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 268/100000, score: -99.45434772149918, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 269/100000, score: -124.16453071161934, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 270/100000, score: -132.78215053600593, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 271/100000, score: -98.26480059896562, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 272/100000, score: -139.92362877769972, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 273/100000, score: -126.03668549283887, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 274/100000, score: -119.65108108630687, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 275/100000, score: -149.8362325857951, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 276/100000, score: -115.50466538142123, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 277/100000, score: -120.42863753291012, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 278/100000, score: -139.01402041015803, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 279/100000, score: -100.17399485418345, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 280/100000, score: -99.24671978383675, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 281/100000, score: -119.52285821550748, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 282/100000, score: -125.894858466817, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 283/100000, score: -115.97643792283569, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 284/100000, score: -130.75053377360996, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 285/100000, score: -132.13158523325671, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 286/100000, score: -140.18490559967015, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 287/100000, score: -99.64483394938759, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 288/100000, score: -100.26622277766883, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 289/100000, score: -95.92429919456595, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 290/100000, score: -101.78868387442698, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 291/100000, score: -98.50817648709317, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 292/100000, score: -102.0359039005083, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 293/100000, score: -111.93025180835829, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 294/100000, score: -114.25581582197348, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 295/100000, score: -99.00154548090704, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 296/100000, score: -111.09200921027647, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 297/100000, score: -115.37290916594868, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 298/100000, score: -148.5985737074937, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 299/100000, score: -117.24557459233958, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 300/100000, score: -115.56693631074971, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 301/100000, score: -117.57261362333006, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 302/100000, score: -179.51487575392474, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 303/100000, score: -111.80617231378832, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 304/100000, score: -150.21409117430989, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 305/100000, score: -185.0690529826837, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 306/100000, score: -118.21611589346855, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 307/100000, score: -147.4815638725829, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 308/100000, score: -98.80899887682719, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 309/100000, score: -128.02689284691402, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 310/100000, score: -174.13379452242714, e: 1.0\n",
            "1600/1600 [==============================] - 0s 7us/step\n",
            "episode: 311/100000, score: -97.7044271322611, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 312/100000, score: -99.4923006075433, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 313/100000, score: -97.23543621447764, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 314/100000, score: -109.40848772548543, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 315/100000, score: -110.4484172888996, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 316/100000, score: -115.23843779794305, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 317/100000, score: -95.8914479241295, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 318/100000, score: -98.03692172337361, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 319/100000, score: -97.65237441964817, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 320/100000, score: -98.12818010868119, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 321/100000, score: -98.44598899305765, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 322/100000, score: -98.88375136774194, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 323/100000, score: -100.22412283952413, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 324/100000, score: -100.56470787650446, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 325/100000, score: -99.89993576650706, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 326/100000, score: -97.98750515670154, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 327/100000, score: -192.62663248370734, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 328/100000, score: -106.36622860751973, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 329/100000, score: -113.4106994798919, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 330/100000, score: -131.55050332021776, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 331/100000, score: -128.12202541081194, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 332/100000, score: -223.2516576418745, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 333/100000, score: -105.32643781906344, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 334/100000, score: -115.18075792338597, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 335/100000, score: -116.15690100517243, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 336/100000, score: -107.0264078969045, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 337/100000, score: -109.88702232374052, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 338/100000, score: -126.62188961924889, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 339/100000, score: -130.65438002832522, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 340/100000, score: -106.49095316468393, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 341/100000, score: -103.45250577584265, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 342/100000, score: -101.02531140746756, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 343/100000, score: -109.9896734451756, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 344/100000, score: -122.93332848570789, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 345/100000, score: -135.002448213643, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 346/100000, score: -98.5840371344218, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 347/100000, score: -97.90850518270626, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 348/100000, score: -115.18075169955476, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 349/100000, score: -185.18871325512958, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 350/100000, score: -111.20536796956696, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 351/100000, score: -103.67834091110069, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 352/100000, score: -119.52689104332006, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 353/100000, score: -152.88501816699534, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 354/100000, score: -107.60325538151137, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 355/100000, score: -128.74606379314767, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 356/100000, score: -109.12771086299504, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 357/100000, score: -141.80207283679306, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 358/100000, score: -134.82724159278212, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 359/100000, score: -118.73274810340698, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 360/100000, score: -135.5977744065346, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 361/100000, score: -98.06531241688539, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 362/100000, score: -98.44706754175702, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 363/100000, score: -97.9979336861534, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 364/100000, score: -101.94282385113033, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 365/100000, score: -101.3309656721867, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 366/100000, score: -101.57702377101225, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 367/100000, score: -99.66359507485268, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 368/100000, score: -98.60581360107544, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 369/100000, score: -103.19097586284494, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 370/100000, score: -115.79325230266005, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 371/100000, score: -160.8707762237678, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 372/100000, score: -105.8160504401323, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 373/100000, score: -102.0912545639263, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 374/100000, score: -102.68415155593246, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 375/100000, score: -126.89592753024715, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 376/100000, score: -115.87917998415277, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 377/100000, score: -116.1793043183477, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 378/100000, score: -128.48916619909986, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 379/100000, score: -103.26545164599207, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 380/100000, score: -107.57502905914396, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 381/100000, score: -102.03914812619571, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 382/100000, score: -107.92548148733968, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 383/100000, score: -114.0930556540386, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 384/100000, score: -209.92439230856075, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 385/100000, score: -99.6168904002157, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 386/100000, score: -116.87049123424757, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 387/100000, score: -110.51583253234382, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 388/100000, score: -127.00700450764721, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 389/100000, score: -101.49893616625012, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 390/100000, score: -121.9254162895249, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 391/100000, score: -110.89631303846247, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 392/100000, score: -111.7084005419616, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 393/100000, score: -124.17300497823778, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 394/100000, score: -100.89997853689825, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 395/100000, score: -132.50039522748452, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 396/100000, score: -97.35942683082283, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 397/100000, score: -120.88534364761458, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 398/100000, score: -110.71402381953283, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 399/100000, score: -102.9271035882746, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 400/100000, score: -114.76269105760555, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 401/100000, score: -161.30130485791108, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 402/100000, score: -116.93045193788343, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 403/100000, score: -116.54990681220875, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 404/100000, score: -119.55617314481191, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 405/100000, score: -100.19508622286651, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 406/100000, score: -101.26453847511716, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 407/100000, score: -98.85764515704338, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 408/100000, score: -99.99822686561961, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 409/100000, score: -99.85115537467502, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 410/100000, score: -97.49893535733344, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 411/100000, score: -101.70844094923781, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 412/100000, score: -102.45535907157706, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 413/100000, score: -107.30795179841076, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 414/100000, score: -100.81205593593066, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 415/100000, score: -98.61463545529494, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 416/100000, score: -104.11730889842954, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 417/100000, score: -178.99288952031768, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 418/100000, score: -106.1732837444622, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 419/100000, score: -105.0909576370536, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 420/100000, score: -99.339037296321, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 421/100000, score: -100.52486605764645, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 422/100000, score: -106.47833434877856, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 423/100000, score: -99.85024306700417, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 424/100000, score: -100.57445291830531, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 425/100000, score: -111.48062749851881, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 426/100000, score: -97.2331287930248, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 427/100000, score: -101.77101591371382, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 428/100000, score: -105.02960634777791, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 429/100000, score: -99.99136119522935, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 430/100000, score: -100.46986923142465, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 431/100000, score: -97.45728827683232, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 432/100000, score: -112.04485847064507, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 433/100000, score: -141.16044046896002, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 434/100000, score: -134.1662728018364, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 435/100000, score: -119.98580122696514, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 436/100000, score: -122.93700455180995, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 437/100000, score: -123.21297311892236, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 438/100000, score: -136.19374719790784, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 439/100000, score: -120.68090188922088, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 440/100000, score: -112.86756397896374, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 441/100000, score: -107.63777645212068, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 442/100000, score: -104.68470315039193, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 443/100000, score: -148.3301485011561, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 444/100000, score: -96.98724607856244, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 445/100000, score: -120.50023892351696, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 446/100000, score: -101.64733231352463, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 447/100000, score: -121.09629804099053, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 448/100000, score: -96.43884900230091, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 449/100000, score: -149.35548822506559, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 450/100000, score: -113.61680013854769, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 451/100000, score: -104.77115103596627, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 452/100000, score: -157.3146699415234, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 453/100000, score: -122.56930048931468, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 454/100000, score: -165.46972648938365, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 455/100000, score: -106.6520417075096, e: 1.0\n",
            "1600/1600 [==============================] - 0s 7us/step\n",
            "episode: 456/100000, score: -126.40041802292836, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 457/100000, score: -115.10584123839077, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 458/100000, score: -113.33425172574306, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 459/100000, score: -129.63699018414798, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 460/100000, score: -137.99131216060016, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 461/100000, score: -101.7917509541801, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 462/100000, score: -101.27877169830232, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 463/100000, score: -100.60742432928218, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 464/100000, score: -100.18157961944432, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 465/100000, score: -109.63966327356067, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 466/100000, score: -105.66161401601757, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 467/100000, score: -121.92302061497602, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 468/100000, score: -113.91293026574752, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 469/100000, score: -125.98676344138069, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 470/100000, score: -124.35591940415259, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 471/100000, score: -109.96265024608104, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 472/100000, score: -98.82936344615338, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 473/100000, score: -99.41558263130428, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 474/100000, score: -113.57187372001812, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 475/100000, score: -187.44987455031662, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 476/100000, score: -97.78913273370182, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 477/100000, score: -107.10690405879836, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 478/100000, score: -103.46394637607999, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 479/100000, score: -109.14751201346425, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 480/100000, score: -97.99707591576727, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 481/100000, score: -106.42706082930908, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 482/100000, score: -106.83740942994018, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 483/100000, score: -108.03359701234106, e: 1.0\n",
            "1600/1600 [==============================] - 0s 7us/step\n",
            "episode: 484/100000, score: -204.54275527057905, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 485/100000, score: -104.78505834555406, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 486/100000, score: -114.13640810046702, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 487/100000, score: -124.55331987951425, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 488/100000, score: -116.7650403594792, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 489/100000, score: -116.54572671592, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 490/100000, score: -122.19876371180773, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 491/100000, score: -99.1459811041199, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 492/100000, score: -97.82682975850817, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 493/100000, score: -104.94242053455548, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 494/100000, score: -131.28193708496585, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 495/100000, score: -123.75143305537839, e: 1.0\n",
            "1600/1600 [==============================] - 0s 7us/step\n",
            "episode: 496/100000, score: -163.54938968857746, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 497/100000, score: -120.34965942722532, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 498/100000, score: -112.69108933882026, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 499/100000, score: -113.55047911525352, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 500/100000, score: -147.90701684609255, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 501/100000, score: -118.70050678650807, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 502/100000, score: -158.8917219907872, e: 1.0\n",
            "1600/1600 [==============================] - 0s 6us/step\n",
            "episode: 503/100000, score: -122.5421748590619, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 504/100000, score: -126.7411911555092, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 505/100000, score: -121.58705928798972, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 506/100000, score: -184.29249764312289, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 507/100000, score: -135.91037863203186, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 508/100000, score: -102.3859063828612, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 509/100000, score: -114.01445260212208, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 510/100000, score: -115.6293055640659, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 511/100000, score: -115.77711537584204, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 512/100000, score: -121.0335206679552, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 513/100000, score: -123.40462470452094, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 514/100000, score: -115.02651177385172, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 515/100000, score: -129.05328429694362, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 516/100000, score: -121.00597733647206, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 517/100000, score: -174.76649183905306, e: 1.0\n",
            "1600/1600 [==============================] - 0s 3us/step\n",
            "episode: 518/100000, score: -113.4593845520673, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 519/100000, score: -204.50217182275907, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 520/100000, score: -100.66935408785261, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 521/100000, score: -99.99199616137587, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 522/100000, score: -98.99393618609223, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 523/100000, score: -107.91440130827594, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 524/100000, score: -114.45109285256298, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 525/100000, score: -115.51578275303662, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 526/100000, score: -127.65912409762592, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 527/100000, score: -106.04026188310883, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 528/100000, score: -131.40314696828503, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 529/100000, score: -102.04665063398365, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 530/100000, score: -98.6950623175424, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 531/100000, score: -100.07239850203399, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 532/100000, score: -98.65660723504764, e: 1.0\n",
            "1600/1600 [==============================] - 0s 5us/step\n",
            "episode: 533/100000, score: -96.35167014196279, e: 1.0\n",
            "1600/1600 [==============================] - 0s 4us/step\n",
            "episode: 534/100000, score: -98.5579134096347, e: 1.0\n",
            "1049/1600 [==================>...........] - ETA: 0s"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}