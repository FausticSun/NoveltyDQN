{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klsFNbTF8j62"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "878QD18d7JDC",
    "outputId": "29b87176-9fb3-4407-9edd-5c400450b76b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "cmake is already the newest version (3.10.2-1ubuntu2).\n",
      "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
      "libopenmpi-dev is already the newest version (2.1.1-8).\n",
      "swig is already the newest version (3.0.12-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.9)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
      "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.10.15)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
      "Requirement already satisfied: box2d_py in /usr/local/lib/python3.6/dist-packages (2.3.5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
    "!pip install gym\n",
    "!pip install box2d_py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jAJVYB0fmsly"
   },
   "source": [
    "# Check if we are allocated a GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dlFeN7DQDovH",
    "outputId": "670bf4ea-a286-417c-eb7a-897c8c2a678c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lN5wW0Y18oMR"
   },
   "source": [
    "# Connect to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "1W29us8L6-Go",
    "outputId": "6a8dc78c-03ea-4a66-a986-ed3e7c40f911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2AooYTa76TDn"
   },
   "source": [
    "# Discrete DQN\n",
    "\n",
    "In this implementation, the actions of the BipedalWalker are discretized into 81 actions, each action being a permutation of {-1,0,1} for each of the four outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UTrPpfVa6TDr"
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zIQ2JJ_p6TDv"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import gym\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle # for saving episodes -> rewards\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cWVPsYUt6TD8"
   },
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCOvLXLK6TD-"
   },
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nx5ibJ9n6TEA"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    This class represents the experience replay buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.capacity = buffer_size\n",
    "        self.len = 0\n",
    "    \n",
    "    def sample(self, n_samples):\n",
    "        batch = []\n",
    "        n_samples = min(self.len, n_samples)\n",
    "        batch = random.sample(self.buffer, n_samples)\n",
    "        \n",
    "        curr_states = np.float32([arr[0] for arr in batch])\n",
    "        actions = np.int32([arr[1] for arr in batch])\n",
    "        rewards = np.float32([arr[2] for arr in batch])\n",
    "        next_states = np.float32([arr[3] for arr in batch])\n",
    "        \n",
    "        return np.array(curr_states), np.array(actions), np.array(rewards), np.array(next_states)\n",
    "    \n",
    "    def add(self, curr_state, action, reward, next_state):\n",
    "        self.buffer.append([curr_state, action, reward, next_state])\n",
    "        self.len = self.len + 1\n",
    "        if (self.len > self.capacity):\n",
    "            self.len = self.capacity\n",
    "    \n",
    "    def processed_add(self, entry):\n",
    "        self.buffer.append(entry)\n",
    "        self.len = self.len + 1\n",
    "        if (self.len > self.capacity):\n",
    "            self.len = self.capacity\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qauv6GHB6TEJ"
   },
   "source": [
    "## Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VadZr9_N6TEL"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, n_inputs, n_output_dim, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self.get_model(n_inputs, n_output_dim)\n",
    "        \n",
    "    def get_model(self, n_input_dim, n_output_dim):\n",
    "        # Output can be sigmoid since we are computing Q-values and not the regressing \n",
    "        # to the actual value of the action. \n",
    "        model = Sequential()\n",
    "        model.add(Dense(16, input_dim=n_input_dim, activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(32, activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(64, activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256, activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(3**n_output_dim, activation='linear'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "        model.compile(\n",
    "            optimizer=Adam(lr=self.learning_rate, ),\n",
    "            loss=\"mse\"\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def predict(self, states):\n",
    "        return self.model.predict(states)\n",
    "    \n",
    "    def fit(self, states, targets, epochs=1, verbose=0):\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-MSGXm06TES"
   },
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6tv0vTh6TEU"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, \n",
    "                 global_buffer_size=30000, local_buffer_size=10000,\n",
    "                 learning_rate=0.001, batch_size=64, gamma=0.9, \n",
    "                 epsilon=0.99, epsilon_decay=0.000001, epsilon_min=0.001,\n",
    "                 name='discreteDQN'):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        self.name = name; \n",
    "        self.n_actions = 3**action_dim\n",
    "        \n",
    "        self.model = DQN(state_dim, action_dim, learning_rate)\n",
    "        self.buffer = ReplayBuffer(global_buffer_size)\n",
    "        self.local_buffer = ReplayBuffer(local_buffer_size)\n",
    "        \n",
    "        self.total_rewards=np.ones(100)*-300\n",
    "    \n",
    "    def get_action_idx(self, state):\n",
    "        if (np.random.rand() < self.epsilon):\n",
    "            return int(random.randrange(self.n_actions))\n",
    "        else:\n",
    "            qvalues = self.model.predict(state);\n",
    "            return np.argmax(qvalues)\n",
    "    \n",
    "    def get_action(self, action_idx):\n",
    "        action = []\n",
    "        #1\n",
    "        output = int(action_idx / 27) - 1\n",
    "        rest = action_idx - 27 * int(action_idx / 27)\n",
    "        action.append(output)\n",
    "        #2\n",
    "        output = int(rest / 9) - 1\n",
    "        rest = rest - 9*int(rest / 9)\n",
    "        action.append(output)\n",
    "        #3\n",
    "        output = int(rest / 3) - 1\n",
    "        rest = rest - 3*int(rest / 3)\n",
    "        action.append(output)\n",
    "        #4\n",
    "        action.append(rest -1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def train_model(self):\n",
    "        states, actions, rewards, next_states = self.buffer.sample(self.batch_size)\n",
    "        qvalues = self.model.predict(next_states)\n",
    "        qvalues = np.float32([np.amax(qvalue) for qvalue in qvalues])\n",
    "        #print(qvalues.shape)\n",
    "        targets = rewards + self.gamma * qvalues\n",
    "        training_targets = self.model.predict(states)\n",
    "        for i in range(self.batch_size):\n",
    "            #print(actions[i])\n",
    "            training_targets[i][actions[i]] = targets[i]\n",
    "        self.model.fit(states, training_targets, epochs=1, verbose=0)\n",
    "        if (self.epsilon > self.epsilon_min):\n",
    "            self.epsilon = self.epsilon - self.epsilon_decay\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        self.local_buffer.add(state, action, reward, next_state)\n",
    "    \n",
    "    def add_local_experience(self, total_reward):\n",
    "        if (np.min(self.total_rewards) < total_reward):\n",
    "            idx = np.argmin(self.total_rewards)\n",
    "            self.total_rewards[idx]=total_reward\n",
    "            \n",
    "            for x in self.local_buffer.buffer:\n",
    "                self.buffer.processed_add(x)\n",
    "        \n",
    "        # Simulate regular experience replay \n",
    "        if np.random.random()<0.01:\n",
    "            for x in self.local_buffer.buffer:\n",
    "                self.buffer.processed_add(x)\n",
    "        \n",
    "        # Clear local memory\n",
    "        self.local_buffer.clear()\n",
    "    \n",
    "    def save_model(self, n_episodes):\n",
    "        GOOGLE_DIR = '/content/gdrive/My Drive/cs4246_project/models/discrete_dqn/trained_models/'\n",
    "        HOME_DIR = './trained_models/'\n",
    "        self.model.model.save(HOME_DIR + self.name + '_ep' + str(n_episodes) + '.h5')\n",
    "        pass\n",
    "    \n",
    "    def load_model(self, model_name):\n",
    "        self.model = keras.models.load_model(model_name)\n",
    "        pass    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gHEuO73V6TEd"
   },
   "source": [
    "\n",
    "## Setup Gym Environment and Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "id": "JVp6RZN76TEg",
    "outputId": "1b159883-80d2-4bed-8c81-131bad36cbcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_47 (Dense)             (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 81)                20817     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)   (None, 81)                0         \n",
      "=================================================================\n",
      "Total params: 40,513\n",
      "Trainable params: 40,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v2')\n",
    "n_state_params = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "agent = DQNAgent(n_state_params, n_actions)\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPISODES = 100000\n",
    "MAX_REWARD = 300\n",
    "MAX_STEPS = env._max_episode_steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nRZVGKeT6TEu"
   },
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388319
    },
    "colab_type": "code",
    "id": "dU6wltrp6TEv",
    "outputId": "bf2746ff-a283-4c4b-cadf-8510dbae5848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/100000, score: -126.02889377483886, e: 0.99\n",
      "episode: 1/100000, score: -115.36892146220865, e: 0.99\n",
      "episode: 2/100000, score: -104.53093081640762, e: 0.99\n",
      "episode: 3/100000, score: -103.48022090635698, e: 0.99\n",
      "episode: 4/100000, score: -127.38397907822392, e: 0.99\n",
      "episode: 5/100000, score: -112.43250657078187, e: 0.99\n"
     ]
    }
   ],
   "source": [
    "for ep in range(MAX_EPISODES):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    for t in range(MAX_STEPS):\n",
    "        state = np.reshape(state, [1, n_state_params])\n",
    "        action_idx = agent.get_action_idx(state)\n",
    "        action = agent.get_action(action_idx)\n",
    "        state = np.reshape(state, [n_state_params])\n",
    "        next_state, reward, isDone, _ = env.step(action)\n",
    "        \n",
    "        agent.store_transition(state, action_idx, reward, next_state)\n",
    "        state = next_state\n",
    "        \n",
    "        total_reward += reward\n",
    "        if (isDone):\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\".format(ep, MAX_EPISODES, total_reward, agent.epsilon))\n",
    "            break\n",
    "        \n",
    "        if (agent.buffer.len > BATCH_SIZE):\n",
    "            agent.train_model()\n",
    "    \n",
    "    agent.add_local_experience(total_reward)\n",
    "    agent.train_model()\n",
    "    \n",
    "    # record rewards dynamically\n",
    "    GOOGLE_FILE = '/content/gdrive/My Drive/cs4246_project/models/discrete_dqn/record.dat'\n",
    "    HOME_FILE = './record.dat'\n",
    "    record_filename = HOME_DIR\n",
    "    data = [ep, total_reward]\n",
    "    with open(record_filename, \"ab\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    if (total_reward > 200):\n",
    "        agent.save_model(ep)\n",
    "        break\n",
    "    \n",
    "    # save model every 100 episodes\n",
    "    if ((ep % 100) == 0):\n",
    "        agent.save_model(ep)\n",
    "        \n",
    "ienv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "gb57315V6TE-",
    "outputId": "a8dd2a4d-a08a-4f7e-f074-b7bd6763aa33"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ea88bf1c9ad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "with open(record_filename, 'rb') as fr:\n",
    "    try:\n",
    "        while True:\n",
    "            data.append(pickle.load(fr))\n",
    "    except EOFError:\n",
    "        pass\n",
    "data = pd.DataFrame(np.array(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVShNuG46TFG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "discrete_dqn.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python (cs4246_project)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
