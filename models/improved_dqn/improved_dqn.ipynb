{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "improved_dqn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "klsFNbTF8j62"
      },
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "878QD18d7JDC",
        "outputId": "79e2bdfe-30e6-46ca-ec53-b1a656554311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install gym\n",
        "!pip install box2d_py\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 9%\r\rReading package lists... 9%\r\rReading package lists... 9%\r\rReading package lists... 9%\r\rReading package lists... 83%\r\rReading package lists... 83%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 88%\r\rReading package lists... 89%\r\rReading package lists... 89%\r\rReading package lists... 89%\r\rReading package lists... 89%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 97%\r\rReading package lists... 97%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "cmake is already the newest version (3.10.2-1ubuntu2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.9)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.10.15)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: box2d_py in /usr/local/lib/python3.6/dist-packages (2.3.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jAJVYB0fmsly"
      },
      "cell_type": "markdown",
      "source": [
        "# Check if we are allocated a GPU\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dlFeN7DQDovH",
        "outputId": "7de8d3b1-fdf6-45f3-e9cd-e5c2d2e80df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lN5wW0Y18oMR"
      },
      "cell_type": "markdown",
      "source": [
        "# Connect to Google Drive"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1W29us8L6-Go",
        "outputId": "ff95c6f6-740d-4b67-c8c0-8fae6f480985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2AooYTa76TDn"
      },
      "cell_type": "markdown",
      "source": [
        "# Discrete DQN\n",
        "\n",
        "In this implementation, the actions of the BipedalWalker are discretized into 81 actions, each action being a permutation of {-1,0,1} for each of the four outputs."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UTrPpfVa6TDr"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Modules"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zIQ2JJ_p6TDv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import gym\n",
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "import pickle # for saving episodes -> rewards\n",
        "\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cWVPsYUt6TD8"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TCOvLXLK6TD-"
      },
      "cell_type": "markdown",
      "source": [
        "### Replay Buffer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Nx5ibJ9n6TEA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    This class represents the experience replay buffer\n",
        "    \"\"\"\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "        self.capacity = buffer_size\n",
        "        self.len = 0\n",
        "    \n",
        "    def sample(self, n_samples):\n",
        "        batch = []\n",
        "        n_samples = min(self.len, n_samples)\n",
        "        batch = random.sample(self.buffer, n_samples)\n",
        "        \n",
        "        curr_states = np.float32([arr[0] for arr in batch])\n",
        "        actions = np.int32([arr[1] for arr in batch])\n",
        "        rewards = np.float32([arr[2] for arr in batch])\n",
        "        next_states = np.float32([arr[3] for arr in batch])\n",
        "        \n",
        "        return np.array(curr_states), np.array(actions), np.array(rewards), np.array(next_states)\n",
        "    \n",
        "    def add(self, curr_state, action, reward, next_state):\n",
        "        self.buffer.append([curr_state, action, reward, next_state])\n",
        "        self.len = self.len + 1\n",
        "        if (self.len > self.capacity):\n",
        "            self.len = self.capacity\n",
        "    \n",
        "    def processed_add(self, entry):\n",
        "        self.buffer.append(entry)\n",
        "        self.len = self.len + 1\n",
        "        if (self.len > self.capacity):\n",
        "            self.len = self.capacity\n",
        "    \n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qauv6GHB6TEJ"
      },
      "cell_type": "markdown",
      "source": [
        "## Q Network"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VadZr9_N6TEL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQN:\n",
        "    def __init__(self, n_inputs, n_output_dim, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = self.get_model(n_inputs, n_output_dim)\n",
        "        \n",
        "    def get_model(self, n_input_dim, n_output_dim):\n",
        "        # Output can be sigmoid since we are computing Q-values and not the regressing \n",
        "        # to the actual value of the action. \n",
        "        model = Sequential()\n",
        "        model.add(Dense(16, input_dim=n_input_dim, activation='linear'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(32, activation='linear'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(64, activation='linear'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256, activation='linear'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(3**n_output_dim, activation='linear'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.summary()\n",
        "        model.compile(\n",
        "            optimizer=Adam(lr=self.learning_rate, ),\n",
        "            loss=\"mse\"\n",
        "        )\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    def predict(self, states):\n",
        "        return self.model.predict(states)\n",
        "    \n",
        "    def fit(self, states, targets, epochs=1, verbose=0):\n",
        "        self.model.fit(states, targets, epochs=1, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "g-MSGXm06TES"
      },
      "cell_type": "markdown",
      "source": [
        "## Create the Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S6tv0vTh6TEU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, \n",
        "                 global_buffer_size=30000, local_buffer_size=10000,\n",
        "                 learning_rate=0.001, batch_size=64, gamma=0.9, \n",
        "                 epsilon=0.99, epsilon_decay=0.001, epsilon_min=0.001,\n",
        "                 name='discreteDQN'):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        \n",
        "        self.name = name; \n",
        "        self.n_actions = 3**action_dim\n",
        "        \n",
        "        self.model = DQN(state_dim, action_dim, learning_rate)\n",
        "        self.buffer = ReplayBuffer(global_buffer_size)\n",
        "        self.local_buffer = ReplayBuffer(local_buffer_size)\n",
        "        \n",
        "        self.total_rewards=np.ones(200)*-300\n",
        "    \n",
        "    def get_action_idx(self, state):\n",
        "        if (np.random.rand() < self.epsilon):\n",
        "            return int(random.randrange(self.n_actions))\n",
        "        else:\n",
        "            qvalues = self.model.predict(state);\n",
        "            return np.argmax(qvalues)\n",
        "    \n",
        "    def get_action(self, action_idx):\n",
        "        action = []\n",
        "        #1\n",
        "        output = int(action_idx / 27) - 1\n",
        "        rest = action_idx - 27 * int(action_idx / 27)\n",
        "        action.append(output)\n",
        "        #2\n",
        "        output = int(rest / 9) - 1\n",
        "        rest = rest - 9*int(rest / 9)\n",
        "        action.append(output)\n",
        "        #3\n",
        "        output = int(rest / 3) - 1\n",
        "        rest = rest - 3*int(rest / 3)\n",
        "        action.append(output)\n",
        "        #4\n",
        "        action.append(rest -1)\n",
        "        \n",
        "        return action\n",
        "    \n",
        "    def train_model(self):\n",
        "        states, actions, rewards, next_states = self.buffer.sample(self.batch_size)\n",
        "        qvalues = self.model.predict(next_states)\n",
        "        qvalues = np.float32([np.amax(qvalue) for qvalue in qvalues])\n",
        "        #print(qvalues.shape)\n",
        "        targets = rewards + self.gamma * qvalues\n",
        "        training_targets = self.model.predict(states)\n",
        "        for i in range(self.batch_size):\n",
        "            #print(actions[i])\n",
        "            training_targets[i][actions[i]] = targets[i]\n",
        "        self.model.fit(states, training_targets, epochs=1, verbose=0)\n",
        "        if (self.epsilon > self.epsilon_min):\n",
        "            self.epsilon = self.epsilon - self.epsilon_decay\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state):\n",
        "        self.local_buffer.add(state, action, reward, next_state)\n",
        "    \n",
        "    def add_local_experience(self, total_reward):\n",
        "        if (np.min(self.total_rewards) < total_reward):\n",
        "            idx = np.argmin(self.total_rewards)\n",
        "            self.total_rewards[idx]=total_reward\n",
        "            \n",
        "            for x in self.local_buffer.buffer:\n",
        "                self.buffer.processed_add(x)\n",
        "        \n",
        "        # Simulate regular experience replay \n",
        "        if np.random.random()<0.01:\n",
        "            for x in self.local_buffer.buffer:\n",
        "                self.buffer.processed_add(x)\n",
        "        \n",
        "        # Clear local memory\n",
        "        self.local_buffer.clear()\n",
        "    \n",
        "    def save_model(self, n_episodes):\n",
        "        GOOGLE_DIR = '/content/gdrive/My Drive/cs4246_project/models/improved_dqn/trained_models/'\n",
        "        HOME_DIR = './trained_models/'\n",
        "        self.model.model.save(GOOGLE_DIR + self.name + '_ep' + str(n_episodes) + '.h5')\n",
        "        pass\n",
        "    \n",
        "    def load_model(self, model_name):\n",
        "        self.model = keras.models.load_model(model_name)\n",
        "        pass    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gHEuO73V6TEd"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Setup Gym Environment and Initialize Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JVp6RZN76TEg",
        "outputId": "d1a381b4-9ded-4196-d417-64b5b52a7b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('BipedalWalker-v2')\n",
        "n_state_params = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.shape[0]\n",
        "agent = DQNAgent(n_state_params, n_actions)\n",
        "BATCH_SIZE = 64\n",
        "MAX_EPISODES = 30000\n",
        "MAX_REWARD = 300\n",
        "MAX_STEPS = env._max_episode_steps\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_16 (Dense)             (None, 16)                400       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 81)                20817     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)   (None, 81)                0         \n",
            "=================================================================\n",
            "Total params: 40,513\n",
            "Trainable params: 40,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nRZVGKeT6TEu"
      },
      "cell_type": "markdown",
      "source": [
        "## Run Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dU6wltrp6TEv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for ep in range(MAX_EPISODES):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    for t in range(MAX_STEPS):\n",
        "        state = np.reshape(state, [1, n_state_params])\n",
        "        action_idx = agent.get_action_idx(state)\n",
        "        action = agent.get_action(action_idx)\n",
        "        state = np.reshape(state, [n_state_params])\n",
        "        next_state, reward, isDone, _ = env.step(action)\n",
        "        \n",
        "        agent.store_transition(state, action_idx, reward, next_state)\n",
        "        state = next_state\n",
        "        \n",
        "        total_reward += reward\n",
        "        if (isDone):\n",
        "            print(\"episode: {}/{}, score: {}, e: {:.2}\".format(ep, MAX_EPISODES, total_reward, agent.epsilon))\n",
        "            break\n",
        "        \n",
        "    \n",
        "    agent.add_local_experience(total_reward)\n",
        "    if (agent.buffer.len > BATCH_SIZE):\n",
        "        agent.train_model()\n",
        "    \n",
        "    # record rewards dynamically\n",
        "    GOOGLE_FILE = '/content/gdrive/My Drive/cs4246_project/models/improved_dqn/record.dat'\n",
        "    HOME_FILE = './record.dat'\n",
        "    record_filename = GOOGLE_FILE\n",
        "    data = [ep, total_reward]\n",
        "    with open(record_filename, \"ab\") as f:\n",
        "        pickle.dump(data, f)\n",
        "    \n",
        "    if (total_reward > 200):\n",
        "        agent.save_model(ep)\n",
        "    \n",
        "    # save model every 10000 episodes\n",
        "    if ((ep % 100) == 0):\n",
        "        agent.save_model(ep)\n",
        "        \n",
        "ienv.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gb57315V6TE-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  import pandas as pd\n",
        "\n",
        "data = []\n",
        "with open(record_filename, 'rb') as fr:\n",
        "    try:\n",
        "        while True:\n",
        "            data.append(pickle.load(fr))\n",
        "    except EOFError:\n",
        "        pass\n",
        "data = pd.DataFrame(np.array(data))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GVShNuG46TFG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}