{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ddpg.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python (cs4246_project)",
      "language": "python",
      "name": "cs4246_project"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "klsFNbTF8j62"
      },
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "878QD18d7JDC",
        "outputId": "154be314-2c52-462f-f3c4-92f5c7677c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4233
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install gym\n",
        "!pip install box2d_py\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 9%\r\rReading package lists... 9%\r\rReading package lists... 9%\r\rReading package lists... 9%\r\rReading package lists... 83%\r\rReading package lists... 83%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 84%\r\rReading package lists... 89%\r\rReading package lists... 89%\r\rReading package lists... 89%\r\rReading package lists... 89%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 97%\r\rReading package lists... 97%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... 99%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  autotools-dev cmake-data file ibverbs-providers libarchive13 libfabric1\n",
            "  libhwloc-dev libhwloc-plugins libhwloc5 libibverbs-dev libibverbs1\n",
            "  libjsoncpp1 libltdl-dev libltdl7 liblzo2-2 libmagic-mgc libmagic1\n",
            "  libnl-3-200 libnl-route-3-200 libnuma-dev libnuma1 libopenmpi2 libpciaccess0\n",
            "  libpsm-infinipath1 librdmacm1 librhash0 libtool libuv1 ocl-icd-libopencl1\n",
            "  openmpi-bin openmpi-common swig3.0\n",
            "Suggested packages:\n",
            "  cmake-doc ninja-build lrzip libhwloc-contrib-plugins libtool-doc openmpi-doc\n",
            "  pciutils autoconf automaken gcj-jdk swig-doc swig-examples swig3.0-examples\n",
            "  swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  autotools-dev cmake cmake-data file ibverbs-providers libarchive13\n",
            "  libfabric1 libhwloc-dev libhwloc-plugins libhwloc5 libibverbs-dev\n",
            "  libibverbs1 libjsoncpp1 libltdl-dev libltdl7 liblzo2-2 libmagic-mgc\n",
            "  libmagic1 libnl-3-200 libnl-route-3-200 libnuma-dev libnuma1 libopenmpi-dev\n",
            "  libopenmpi2 libpciaccess0 libpsm-infinipath1 librdmacm1 librhash0 libtool\n",
            "  libuv1 ocl-icd-libopencl1 openmpi-bin openmpi-common swig swig3.0\n",
            "0 upgraded, 35 newly installed, 0 to remove and 5 not upgraded.\n",
            "Need to get 11.5 MB of archives.\n",
            "After this operation, 58.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblzo2-2 amd64 2.08-1.2 [48.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.1 [184 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.1 [68.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.1 [22.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnuma1 amd64 2.0.11-2.1 [21.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 autotools-dev all 20180224.1 [39.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 cmake-data all 3.10.2-1ubuntu2 [1,331 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libarchive13 amd64 3.2.2-3.1ubuntu0.1 [289 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjsoncpp1 amd64 1.7.4-3 [73.6 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 librhash0 amd64 1.3.6-2 [78.1 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libuv1 amd64 1.18.0-3 [64.4 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 cmake amd64 3.10.2-1ubuntu2 [3,138 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnl-3-200 amd64 3.2.29-0ubuntu3 [52.8 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnl-route-3-200 amd64 3.2.29-0ubuntu3 [146 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libibverbs1 amd64 17.1-1 [44.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 ibverbs-providers amd64 17.1-1 [160 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libpsm-infinipath1 amd64 3.3+20.604758e7-5 [174 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 librdmacm1 amd64 17.1-1 [56.1 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libfabric1 amd64 1.5.3-1 [302 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libltdl7 amd64 2.4.6-2 [38.8 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libltdl-dev amd64 2.4.6-2 [162 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpciaccess0 amd64 0.14-1 [17.9 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtool all 2.4.6-2 [194 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libhwloc5 amd64 1.11.9-1 [105 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 ocl-icd-libopencl1 amd64 2.2.11-1ubuntu1 [30.3 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libhwloc-plugins amd64 1.11.9-1 [12.5 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenmpi2 amd64 2.1.1-8 [2,056 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/universe amd64 openmpi-common all 2.1.1-8 [140 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/universe amd64 openmpi-bin amd64 2.1.1-8 [88.2 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnuma-dev amd64 2.0.11-2.1 [32.2 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libhwloc-dev amd64 1.11.9-1 [167 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic/main amd64 libibverbs-dev amd64 17.1-1 [103 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenmpi-dev amd64 2.1.1-8 [925 kB]\n",
            "Fetched 11.5 MB in 4s (2,575 kB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "(Reading database ... 22280 files and directories currently installed.)\n",
            "Preparing to unpack .../00-liblzo2-2_2.08-1.2_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.08-1.2) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "Preparing to unpack .../01-libmagic-mgc_1%3a5.32-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../02-libmagic1_1%3a5.32-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.1) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../03-file_1%3a5.32-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libnuma1:amd64.\n",
            "Preparing to unpack .../04-libnuma1_2.0.11-2.1_amd64.deb ...\n",
            "Unpacking libnuma1:amd64 (2.0.11-2.1) ...\n",
            "Selecting previously unselected package autotools-dev.\n",
            "Preparing to unpack .../05-autotools-dev_20180224.1_all.deb ...\n",
            "Unpacking autotools-dev (20180224.1) ...\n",
            "Selecting previously unselected package cmake-data.\n",
            "Preparing to unpack .../06-cmake-data_3.10.2-1ubuntu2_all.deb ...\n",
            "Unpacking cmake-data (3.10.2-1ubuntu2) ...\n",
            "Selecting previously unselected package libarchive13:amd64.\n",
            "Preparing to unpack .../07-libarchive13_3.2.2-3.1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libarchive13:amd64 (3.2.2-3.1ubuntu0.1) ...\n",
            "Selecting previously unselected package libjsoncpp1:amd64.\n",
            "Preparing to unpack .../08-libjsoncpp1_1.7.4-3_amd64.deb ...\n",
            "Unpacking libjsoncpp1:amd64 (1.7.4-3) ...\n",
            "Selecting previously unselected package librhash0:amd64.\n",
            "Preparing to unpack .../09-librhash0_1.3.6-2_amd64.deb ...\n",
            "Unpacking librhash0:amd64 (1.3.6-2) ...\n",
            "Selecting previously unselected package libuv1:amd64.\n",
            "Preparing to unpack .../10-libuv1_1.18.0-3_amd64.deb ...\n",
            "Unpacking libuv1:amd64 (1.18.0-3) ...\n",
            "Selecting previously unselected package cmake.\n",
            "Preparing to unpack .../11-cmake_3.10.2-1ubuntu2_amd64.deb ...\n",
            "Unpacking cmake (3.10.2-1ubuntu2) ...\n",
            "Selecting previously unselected package libnl-3-200:amd64.\n",
            "Preparing to unpack .../12-libnl-3-200_3.2.29-0ubuntu3_amd64.deb ...\n",
            "Unpacking libnl-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
            "Selecting previously unselected package libnl-route-3-200:amd64.\n",
            "Preparing to unpack .../13-libnl-route-3-200_3.2.29-0ubuntu3_amd64.deb ...\n",
            "Unpacking libnl-route-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
            "Selecting previously unselected package libibverbs1:amd64.\n",
            "Preparing to unpack .../14-libibverbs1_17.1-1_amd64.deb ...\n",
            "Unpacking libibverbs1:amd64 (17.1-1) ...\n",
            "Selecting previously unselected package ibverbs-providers:amd64.\n",
            "Preparing to unpack .../15-ibverbs-providers_17.1-1_amd64.deb ...\n",
            "Unpacking ibverbs-providers:amd64 (17.1-1) ...\n",
            "Selecting previously unselected package libpsm-infinipath1.\n",
            "Preparing to unpack .../16-libpsm-infinipath1_3.3+20.604758e7-5_amd64.deb ...\n",
            "Unpacking libpsm-infinipath1 (3.3+20.604758e7-5) ...\n",
            "Selecting previously unselected package librdmacm1:amd64.\n",
            "Preparing to unpack .../17-librdmacm1_17.1-1_amd64.deb ...\n",
            "Unpacking librdmacm1:amd64 (17.1-1) ...\n",
            "Selecting previously unselected package libfabric1.\n",
            "Preparing to unpack .../18-libfabric1_1.5.3-1_amd64.deb ...\n",
            "Unpacking libfabric1 (1.5.3-1) ...\n",
            "Selecting previously unselected package libltdl7:amd64.\n",
            "Preparing to unpack .../19-libltdl7_2.4.6-2_amd64.deb ...\n",
            "Unpacking libltdl7:amd64 (2.4.6-2) ...\n",
            "Selecting previously unselected package libltdl-dev:amd64.\n",
            "Preparing to unpack .../20-libltdl-dev_2.4.6-2_amd64.deb ...\n",
            "Unpacking libltdl-dev:amd64 (2.4.6-2) ...\n",
            "Selecting previously unselected package libpciaccess0:amd64.\n",
            "Preparing to unpack .../21-libpciaccess0_0.14-1_amd64.deb ...\n",
            "Unpacking libpciaccess0:amd64 (0.14-1) ...\n",
            "Selecting previously unselected package libtool.\n",
            "Preparing to unpack .../22-libtool_2.4.6-2_all.deb ...\n",
            "Unpacking libtool (2.4.6-2) ...\n",
            "Selecting previously unselected package libhwloc5:amd64.\n",
            "Preparing to unpack .../23-libhwloc5_1.11.9-1_amd64.deb ...\n",
            "Unpacking libhwloc5:amd64 (1.11.9-1) ...\n",
            "Selecting previously unselected package ocl-icd-libopencl1:amd64.\n",
            "Preparing to unpack .../24-ocl-icd-libopencl1_2.2.11-1ubuntu1_amd64.deb ...\n",
            "Unpacking ocl-icd-libopencl1:amd64 (2.2.11-1ubuntu1) ...\n",
            "Selecting previously unselected package libhwloc-plugins.\n",
            "Preparing to unpack .../25-libhwloc-plugins_1.11.9-1_amd64.deb ...\n",
            "Unpacking libhwloc-plugins (1.11.9-1) ...\n",
            "Selecting previously unselected package libopenmpi2:amd64.\n",
            "Preparing to unpack .../26-libopenmpi2_2.1.1-8_amd64.deb ...\n",
            "Unpacking libopenmpi2:amd64 (2.1.1-8) ...\n",
            "Selecting previously unselected package openmpi-common.\n",
            "Preparing to unpack .../27-openmpi-common_2.1.1-8_all.deb ...\n",
            "Unpacking openmpi-common (2.1.1-8) ...\n",
            "Selecting previously unselected package openmpi-bin.\n",
            "Preparing to unpack .../28-openmpi-bin_2.1.1-8_amd64.deb ...\n",
            "Unpacking openmpi-bin (2.1.1-8) ...\n",
            "Selecting previously unselected package swig3.0.\n",
            "Preparing to unpack .../29-swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../30-swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Selecting previously unselected package libnuma-dev:amd64.\n",
            "Preparing to unpack .../31-libnuma-dev_2.0.11-2.1_amd64.deb ...\n",
            "Unpacking libnuma-dev:amd64 (2.0.11-2.1) ...\n",
            "Selecting previously unselected package libhwloc-dev:amd64.\n",
            "Preparing to unpack .../32-libhwloc-dev_1.11.9-1_amd64.deb ...\n",
            "Unpacking libhwloc-dev:amd64 (1.11.9-1) ...\n",
            "Selecting previously unselected package libibverbs-dev:amd64.\n",
            "Preparing to unpack .../33-libibverbs-dev_17.1-1_amd64.deb ...\n",
            "Unpacking libibverbs-dev:amd64 (17.1-1) ...\n",
            "Selecting previously unselected package libopenmpi-dev.\n",
            "Preparing to unpack .../34-libopenmpi-dev_2.1.1-8_amd64.deb ...\n",
            "Unpacking libopenmpi-dev (2.1.1-8) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up libuv1:amd64 (1.18.0-3) ...\n",
            "Setting up libnuma1:amd64 (2.0.11-2.1) ...\n",
            "Setting up cmake-data (3.10.2-1ubuntu2) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.1) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.1) ...\n",
            "Setting up librhash0:amd64 (1.3.6-2) ...\n",
            "Setting up libpsm-infinipath1 (3.3+20.604758e7-5) ...\n",
            "update-alternatives: using /usr/lib/libpsm1/libpsm_infinipath.so.1.16 to provide /usr/lib/x86_64-linux-gnu/libpsm_infinipath.so.1 (libpsm_infinipath.so.1) in auto mode\n",
            "Setting up openmpi-common (2.1.1-8) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up autotools-dev (20180224.1) ...\n",
            "Setting up libltdl7:amd64 (2.4.6-2) ...\n",
            "Setting up libpciaccess0:amd64 (0.14-1) ...\n",
            "Setting up libnuma-dev:amd64 (2.0.11-2.1) ...\n",
            "Setting up ocl-icd-libopencl1:amd64 (2.2.11-1ubuntu1) ...\n",
            "Setting up libnl-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
            "Setting up liblzo2-2:amd64 (2.08-1.2) ...\n",
            "Setting up libjsoncpp1:amd64 (1.7.4-3) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Setting up libltdl-dev:amd64 (2.4.6-2) ...\n",
            "Setting up libarchive13:amd64 (3.2.2-3.1ubuntu0.1) ...\n",
            "Setting up libnl-route-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
            "Setting up file (1:5.32-2ubuntu0.1) ...\n",
            "Setting up libhwloc5:amd64 (1.11.9-1) ...\n",
            "Setting up libhwloc-plugins (1.11.9-1) ...\n",
            "Setting up cmake (3.10.2-1ubuntu2) ...\n",
            "Setting up libtool (2.4.6-2) ...\n",
            "Setting up libibverbs1:amd64 (17.1-1) ...\n",
            "Setting up libhwloc-dev:amd64 (1.11.9-1) ...\n",
            "Setting up librdmacm1:amd64 (17.1-1) ...\n",
            "Setting up ibverbs-providers:amd64 (17.1-1) ...\n",
            "Setting up libibverbs-dev:amd64 (17.1-1) ...\n",
            "Setting up libfabric1 (1.5.3-1) ...\n",
            "Setting up libopenmpi2:amd64 (2.1.1-8) ...\n",
            "Setting up libopenmpi-dev (2.1.1-8) ...\n",
            "update-alternatives: using /usr/lib/x86_64-linux-gnu/openmpi/include to provide /usr/include/mpi (mpi) in auto mode\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpicc.1.gz because associated file /usr/share/man/man1/mpicc.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpic++.1.gz because associated file /usr/share/man/man1/mpic++.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpicxx.1.gz because associated file /usr/share/man/man1/mpicxx.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpiCC.1.gz because associated file /usr/share/man/man1/mpiCC.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpif77.1.gz because associated file /usr/share/man/man1/mpif77.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpif90.1.gz because associated file /usr/share/man/man1/mpif90.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpifort.1.gz because associated file /usr/share/man/man1/mpifort.openmpi.1.gz (of link group mpi) doesn't exist\n",
            "Setting up openmpi-bin (2.1.1-8) ...\n",
            "update-alternatives: using /usr/bin/mpirun.openmpi to provide /usr/bin/mpirun (mpirun) in auto mode\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpirun.1.gz because associated file /usr/share/man/man1/mpirun.openmpi.1.gz (of link group mpirun) doesn't exist\n",
            "update-alternatives: warning: skip creation of /usr/share/man/man1/mpiexec.1.gz because associated file /usr/share/man/man1/mpiexec.openmpi.1.gz (of link group mpirun) doesn't exist\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Collecting gym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/22/4ff09745ade385ffe707fb5f053548f0f6a6e7d5e98a2b9d6c07f5b931a7/gym-0.10.9.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python2.7/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python2.7/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python2.7/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from gym) (1.11.0)\n",
            "Collecting pyglet>=1.2.0 (from gym)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 17.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.0->gym) (2018.10.15)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python2.7/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6c/3a/0e/b86dee98876bb56cdb482cc1f72201035e46d1baf69d10d028\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "Successfully installed gym-0.10.9 pyglet-1.3.2\n",
            "Collecting box2d_py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/5a/ad8d3ef9c13d5afcc1e44a77f11792ee717f6727b3320bddbc607e935e2a/box2d-py-2.3.5.tar.gz (374kB)\n",
            "\u001b[K    100% |████████████████████████████████| 378kB 18.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Running setup.py bdist_wheel for box2d-py ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/87/54/2d/43b2f9aab5e6830d45928727066257ae6f5fc61f758170eef2\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jAJVYB0fmsly"
      },
      "cell_type": "markdown",
      "source": [
        "# Check if we are allocated a GPU\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dlFeN7DQDovH",
        "outputId": "c1bd2b1e-3cd3-43b8-8466-00675858be26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lN5wW0Y18oMR"
      },
      "cell_type": "markdown",
      "source": [
        "# Connect to Google Drive"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1W29us8L6-Go",
        "outputId": "b2f5acab-4978-4c64-d9f2-711f3dab23d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2AooYTa76TDn"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Deterministic Policy Gradient\n",
        "\n",
        "In this implementation, the actions of the BipedalWalker are discretized into 81 actions, each action being a permutation of {-1,0,1} for each of the four outputs."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UTrPpfVa6TDr"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Modules"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zIQ2JJ_p6TDv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import gym\n",
        "import os\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, BatchNormalization, Input, Add, LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "import pickle # for saving episodes -> rewards\n",
        "\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from tqdm import tqdm, trange, tnrange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cWVPsYUt6TD8"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the Model"
      ]
    },
    {
      "metadata": {
        "id": "F150YH8nTBwj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create Noise"
      ]
    },
    {
      "metadata": {
        "id": "0_iIHWpCTBwk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py, which is\n",
        "# based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
        "class OrnsteinUhlenbeckActionNoise:\n",
        "    def __init__(self, mu, sigma=0.5, theta=.20, dt=1e-2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TCOvLXLK6TD-"
      },
      "cell_type": "markdown",
      "source": [
        "### Replay Buffer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Nx5ibJ9n6TEA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    This class represents the experience replay buffer\n",
        "    \"\"\"\n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "        self.capacity = buffer_size\n",
        "        self.len = 0\n",
        "    \n",
        "    def sample(self, n_samples):\n",
        "        batch = []\n",
        "        n_samples = min(self.len, n_samples)\n",
        "        batch = random.sample(self.buffer, n_samples)\n",
        "        \n",
        "        curr_states = np.float32([arr[0] for arr in batch])\n",
        "        actions = np.int32([arr[1] for arr in batch])\n",
        "        rewards = np.float32([arr[2] for arr in batch])\n",
        "        done = np.bool_([arr[3] for arr in batch])\n",
        "        next_states = np.float32([arr[4] for arr in batch])\n",
        "        \n",
        "        return np.array(curr_states), np.array(actions), np.array(rewards), np.array(done), np.array(next_states)\n",
        "    \n",
        "    def add(self, curr_state, action, reward, done, next_state):\n",
        "        self.buffer.append([curr_state, action, reward, done, next_state])\n",
        "        self.len = self.len + 1\n",
        "        if (self.len > self.capacity):\n",
        "            self.len = self.capacity\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9L59sqSVTBwr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Actor Network"
      ]
    },
    {
      "metadata": {
        "id": "_pDbVi2mTBws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Actor():\n",
        "    \"\"\"\n",
        "    Input to the network is the state, output is the action\n",
        "    under a deterministic policy.\n",
        "    The output layer activation is a tanh to keep the action\n",
        "    between -action_bound and action_bound\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, learning_rate, tau):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.tau = tau  # tau is the update rate for the target network - allowing it to slowly chase\n",
        "        \n",
        "        # Actor Network\n",
        "        self.model = self.create_actor_network()\n",
        "        self.actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
        "        \n",
        "        print(self.model)\n",
        "        # Target Network\n",
        "        self.target = self.create_actor_network()\n",
        "\n",
        "    def create_actor_network(self):\n",
        "        model = Sequential()\n",
        "        #print(model)\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dense(256, input_dim=self.state_dim, activation='linear'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        #print(model)\n",
        "        model.add(Dense(256, activation='linear'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        #print(model)\n",
        "        model.add(Dense(256, activation='linear'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        #print(model)\n",
        "        # output of bipedal walker is between -1 and 1\n",
        "        model.add(Dense(self.action_dim, activation='tanh'))\n",
        "        #print(model)\n",
        "        model.compile(\n",
        "            optimizer=Adam(lr=self.learning_rate, ),\n",
        "            loss=\"mse\"\n",
        "        )\n",
        "        #model.summary()\n",
        "        print(model)\n",
        "        \n",
        "        return model\n",
        "\n",
        "    def fit(self, states, action_gradients):\n",
        "        self.model.fit(states, action_gradients, verbose=0)\n",
        "    \n",
        "    def noisy_predict(self, inputs):\n",
        "        return self.model.predict(inputs, verbose=0) + self.actor_noise() \n",
        "    \n",
        "    def predict(self, states):\n",
        "        return self.model.predict(states, verbose=0)\n",
        "\n",
        "    def predict_target(self, states):\n",
        "        return self.target.predict(states, verbose=0)\n",
        "    \n",
        "    def update_target(self):\n",
        "        \"\"\"\n",
        "        Soft update of target network\n",
        "        \"\"\"\n",
        "        self.target.set_weights(\n",
        "            [x[0] * self.tau + x[1] * (1 - self.tau) for x in zip(self.model.get_weights(), self.target.get_weights())]\n",
        "        )\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qauv6GHB6TEJ"
      },
      "cell_type": "markdown",
      "source": [
        "## Critic Network"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VadZr9_N6TEL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Critic():\n",
        "    \"\"\"\n",
        "    This network takes in 2 inputs the state and action, \n",
        "    the output is the Q(s,a)\n",
        "    The input action is given by the Actor\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, sess, state_dim, action_dim, \n",
        "                 learning_rate, tau, gamma):\n",
        "        self.sess = sess\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.tau = tau\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # Session needed to grab action gradients\n",
        "        K.set_session(sess)\n",
        "\n",
        "        # Create networks\n",
        "        self.model, self.state, self.action = self.create_critic_network()\n",
        "        self.target, self.target_action, self.target_state = self.create_critic_network()\n",
        "        \n",
        "        self.action_grads = tf.gradients(self.model.output, self.action)\n",
        "        self.sess.run(tf.initialize_all_variables())\n",
        "\n",
        "    def create_critic_network(self):\n",
        "        state_inputs = Input(shape=(self.state_dim,), name='state_input')\n",
        "        action_inputs = Input(shape=(self.action_dim,), name='action_input')\n",
        "        \n",
        "        state_net = BatchNormalization()(state_inputs)\n",
        "        \n",
        "        state_net = Dense(256)(state_net)\n",
        "        state_net = BatchNormalization()(state_net)\n",
        "        state_net = LeakyReLU(alpha=0.2)(state_net)\n",
        "        \n",
        "        state_net = Dense(256)(state_net)\n",
        "        state_net = BatchNormalization()(state_net)\n",
        "        state_net = LeakyReLU(alpha=0.2)(state_net)\n",
        "        \n",
        "        state_net = Dense(256)(state_net)\n",
        "        state_net = BatchNormalization()(state_net)\n",
        "        state_net = LeakyReLU(alpha=0.2)(state_net)\n",
        "        state_net = Dense(300)(state_net)\n",
        "        \n",
        "        # Actions do not need to be normalized - already between -1 and 1\n",
        "        action_net = Dense(300)(action_inputs)\n",
        "        \n",
        "        # Combine state_net and action_net\n",
        "        net = Add()([state_net, action_net])\n",
        "        net = Dense(300)(net)\n",
        "        net = BatchNormalization()(net)\n",
        "        net = LeakyReLU(alpha=0.2)(net)\n",
        "        net = Dense(1)(net)\n",
        "        \n",
        "        model = keras.models.Model(inputs=[state_inputs, action_inputs], outputs=net)\n",
        "        model.compile(\n",
        "            optimizer=Adam(lr=self.learning_rate),\n",
        "            loss=\"mse\"\n",
        "        )\n",
        "        model.summary()\n",
        "        \n",
        "        return model, state_inputs, action_inputs\n",
        "\n",
        "    def fit(self, states, actions, predicted_q_values):\n",
        "        self.model.fit({'state_input': states, 'action_input': actions}, predicted_q_values, verbose=0)\n",
        "\n",
        "    def predict(self, states, action):\n",
        "        return self.model.predict({'state_input': states, 'action_input': actions}, verbose=0)\n",
        "\n",
        "    def predict_target(self, states, actions):\n",
        "        return self.target.predict({'state_input': states, 'action_input': actions}, verbose=0)\n",
        "    \n",
        "    def get_action_gradients(self, states, actions):\n",
        "        return self.sess.run(\n",
        "            self.action_grads,\n",
        "            feed_dict={\n",
        "                self.state: states,\n",
        "                self.action: actions\n",
        "            })[0]\n",
        "    \n",
        "    def update_target(self):\n",
        "        self.target.set_weights(\n",
        "            [x[0] * self.tau + x[1] * (1 - self.tau) for x in zip(self.model.get_weights(), self.target.get_weights())]\n",
        "        )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "g-MSGXm06TES"
      },
      "cell_type": "markdown",
      "source": [
        "## Create the Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S6tv0vTh6TEU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DDPGAgent:\n",
        "    def __init__(self, sess, state_dim, action_dim, buffer_size=30000, \n",
        "                 learning_rate=0.001, batch_size=64, gamma=0.9, \n",
        "                 epsilon=1.00, epsilon_decay=0.99999, epsilon_min=0.001,\n",
        "                 name='DDPG', tau=0.001):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.tau = tau\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        \n",
        "        self.name = name;\n",
        "        \n",
        "        self.actor = Actor(state_dim, action_dim, learning_rate, tau)\n",
        "        self.critic = Critic(sess, state_dim, action_dim, learning_rate, tau, gamma)\n",
        "        \n",
        "        self.buffer = ReplayBuffer(buffer_size)\n",
        "        \n",
        "    \n",
        "    def get_noisy_action(self, states):\n",
        "        states = np.reshape(states, (1, self.state_dim))\n",
        "        return self.actor.noisy_predict(states)\n",
        "    \n",
        "    \n",
        "    def train_model(self):     \n",
        "        if (self.buffer.len > self.batch_size):\n",
        "            states, actions, rewards, done, next_states = self.buffer.sample(self.batch_size)\n",
        "            target_q = self.critic.predict_target(next_states, actions)\n",
        "            \n",
        "            targets = []\n",
        "            for i in range(self.batch_size):\n",
        "                if (done[i]):\n",
        "                    targets.append(rewards[i])\n",
        "                else:\n",
        "                    targets.append(rewards[i] + self.gamma + target_q[i])\n",
        "            \n",
        "            # Update critic\n",
        "            self.critic.fit(states, actions, np.reshape(targets, (-1,1)))\n",
        "            \n",
        "            actor_actions = self.actor.predict(states)\n",
        "            action_gradients = self.critic.get_action_gradients(states, actor_actions)\n",
        "            self.actor.fit(states, action_gradients)\n",
        "            \n",
        "            self.actor.update_target()\n",
        "            self.critic.update_target()\n",
        "    \n",
        "    def store_transition(self, state, action, reward, done, next_state):\n",
        "        self.buffer.add(state, action, reward, done, next_state)\n",
        "    \n",
        "    def save_model(self, n_episodes):\n",
        "        GOOGLE_DIR = '/content/gdrive/My Drive/cs4246_project/models/ddpg/trained_models/'\n",
        "        HOME_DIR = './trained_models/'\n",
        "        self.actor.model.save(GOOGLE_DIR + 'actor' + '_ep' + str(n_episodes) + '.h5')\n",
        "        self.critic.model.save(GOOGLE_DIR + 'critic' + '_ep' + str(n_episodes) + '.h5')\n",
        "        pass\n",
        "    \n",
        "    def load_model(self, model_name):\n",
        "        self.model = keras.models.load_model(model_name)\n",
        "        pass\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gHEuO73V6TEd"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup Gym Environment and Initialize Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JVp6RZN76TEg",
        "outputId": "cd697cb6-903b-4834-e393-4072b371d677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1666
        }
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('BipedalWalker-v2')\n",
        "n_state_params = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.shape[0]\n",
        "\n",
        "# allow GPU optimization\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.allow_growth = True\n",
        "#sess = tf.Session(config=config)\n",
        "sess = tf.Session()\n",
        "\n",
        "agent = DDPGAgent(sess, n_state_params, n_actions)\n",
        "BATCH_SIZE = 64\n",
        "MAX_EPISODES = 100000\n",
        "MAX_REWARD = 300\n",
        "MAX_STEPS = env._max_episode_steps\n"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "<keras.engine.sequential.Sequential object at 0x7f276fa0d350>\n",
            "<keras.engine.sequential.Sequential object at 0x7f276fa0d350>\n",
            "<keras.engine.sequential.Sequential object at 0x7f2759e2c6d0>\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "state_input (InputLayer)        (None, 24)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_372 (BatchN (None, 24)           96          state_input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_510 (Dense)               (None, 256)          6400        batch_normalization_372[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_373 (BatchN (None, 256)          1024        dense_510[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_329 (LeakyReLU)     (None, 256)          0           batch_normalization_373[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_511 (Dense)               (None, 256)          65792       leaky_re_lu_329[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_374 (BatchN (None, 256)          1024        dense_511[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_330 (LeakyReLU)     (None, 256)          0           batch_normalization_374[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_512 (Dense)               (None, 256)          65792       leaky_re_lu_330[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_375 (BatchN (None, 256)          1024        dense_512[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_331 (LeakyReLU)     (None, 256)          0           batch_normalization_375[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "action_input (InputLayer)       (None, 4)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_513 (Dense)               (None, 300)          77100       leaky_re_lu_331[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_514 (Dense)               (None, 300)          1500        action_input[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 300)          0           dense_513[0][0]                  \n",
            "                                                                 dense_514[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_515 (Dense)               (None, 300)          90300       add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_376 (BatchN (None, 300)          1200        dense_515[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_332 (LeakyReLU)     (None, 300)          0           batch_normalization_376[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_516 (Dense)               (None, 1)            301         leaky_re_lu_332[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 311,553\n",
            "Trainable params: 309,369\n",
            "Non-trainable params: 2,184\n",
            "__________________________________________________________________________________________________\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "state_input (InputLayer)        (None, 24)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_377 (BatchN (None, 24)           96          state_input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_517 (Dense)               (None, 256)          6400        batch_normalization_377[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_378 (BatchN (None, 256)          1024        dense_517[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_333 (LeakyReLU)     (None, 256)          0           batch_normalization_378[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_518 (Dense)               (None, 256)          65792       leaky_re_lu_333[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_379 (BatchN (None, 256)          1024        dense_518[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_334 (LeakyReLU)     (None, 256)          0           batch_normalization_379[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_519 (Dense)               (None, 256)          65792       leaky_re_lu_334[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_380 (BatchN (None, 256)          1024        dense_519[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_335 (LeakyReLU)     (None, 256)          0           batch_normalization_380[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "action_input (InputLayer)       (None, 4)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_520 (Dense)               (None, 300)          77100       leaky_re_lu_335[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_521 (Dense)               (None, 300)          1500        action_input[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 300)          0           dense_520[0][0]                  \n",
            "                                                                 dense_521[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_522 (Dense)               (None, 300)          90300       add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_381 (BatchN (None, 300)          1200        dense_522[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_336 (LeakyReLU)     (None, 300)          0           batch_normalization_381[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_523 (Dense)               (None, 1)            301         leaky_re_lu_336[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 311,553\n",
            "Trainable params: 309,369\n",
            "Non-trainable params: 2,184\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nRZVGKeT6TEu"
      },
      "cell_type": "markdown",
      "source": [
        "## Run Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dU6wltrp6TEv",
        "outputId": "212f5933-edac-4b7a-f09d-b3f9f8ce708c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        }
      },
      "cell_type": "code",
      "source": [
        "for ep in trange(MAX_EPISODES, desc='Episodes'):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    for t in trange(MAX_STEPS, desc='Ep{} Steps'.format(ep)):\n",
        "        state = np.reshape(state, [1, n_state_params])\n",
        "        actions = agent.get_noisy_action(state)\n",
        "        state = np.reshape(state, [n_state_params])\n",
        "        next_state, reward, isDone, _ = env.step(actions[0])\n",
        "        \n",
        "        agent.store_transition(state, actions[0], reward, isDone, next_state)\n",
        "        state = next_state\n",
        "        agent.train_model()\n",
        "        total_reward += reward\n",
        "        if (isDone):\n",
        "            print(\"episode: {}/{}, score: {}, e: {:.2}\".format(ep, MAX_EPISODES, total_reward, agent.epsilon))\n",
        "            break\n",
        "        \n",
        "        if (agent.buffer.len > BATCH_SIZE):\n",
        "            agent.train_model()\n",
        "    \n",
        "    # record rewards dynamically\n",
        "    GOOGLE_FILE = '/content/gdrive/My Drive/cs4246_project/models/ddpg/record.dat'\n",
        "    HOME_FILE = './record.dat'\n",
        "    record_filename = HOME_FILE\n",
        "    data = [ep, total_reward]\n",
        "    with open(record_filename, \"a+\") as f:\n",
        "        pickle.dump(data, f)\n",
        "    \n",
        "    if (total_reward > 200):\n",
        "        agent.save_model(ep)\n",
        "        break\n",
        "    \n",
        "    # save model every 10000 episodes\n",
        "    if ((ep % 10000) == 0):\n",
        "        agent.save_model(ep)\n",
        "    \n",
        "    if (total_reward > 200):\n",
        "        agent.save_model(ep)\n",
        "        break\n",
        "    \n",
        "    # save model every 100 episodes\n",
        "    if ((ep % 100) == 0):\n",
        "        agent.save_model(ep)\n",
        "        \n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Episodes:   0%|          | 0/100000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Ep0 Steps:   0%|          | 0/1600 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Ep0 Steps:   0%|          | 1/1600 [00:23<10:34:09, 23.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Ep0 Steps:   1%|          | 14/1600 [00:23<7:20:21, 16.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Ep0 Steps:   2%|▏         | 30/1600 [00:24<5:05:11, 11.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Ep0 Steps:   3%|▎         | 48/1600 [00:24<3:31:13,  8.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gb57315V6TE-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "with open(record_filename, 'rb') as fr:\n",
        "    try:\n",
        "        while True:\n",
        "            data.append(pickle.load(fr))\n",
        "    except EOFError:\n",
        "        pass\n",
        "data = pd.DataFrame(np.array(data))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GVShNuG46TFG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eJA6TFALTBxG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GnrTcjKCTBxK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}